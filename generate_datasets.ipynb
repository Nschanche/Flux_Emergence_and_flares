{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sunpy.net import hek\n",
    "from shapely import wkt\n",
    "from sunpy.time import parse_time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = hek.HEKClient()\n",
    "start_time = pd.to_datetime('2010/06/01 00:00')\n",
    "end_time = pd.to_datetime('2016/06/01 00:00')\n",
    "fl_detective = client.query(hek.attrs.Time(start_time.isoformat(),\n",
    "                                                 end_time.isoformat()),\n",
    "                                  hek.attrs.EventType('FL'),\n",
    "                                  hek.attrs.FRM.Name == 'Flare Detective - Trigger Module')\n",
    "fl_detective = pd.DataFrame(fl_detective)\n",
    "fl_detective.to_csv('raw_detective.csv', index = False)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fl_SSW = client.query(hek.attrs.Time(start_time.isoformat(),end_time.isoformat()),\n",
    "                                  hek.attrs.EventType('FL'), hek.attrs.FRM.Name == 'SSW Latest Events', \n",
    "                      hek.attrs.FL.GOESCls != None)\n",
    "fl_SSW = pd.DataFrame(fl_SSW)\n",
    "fl_SSW.to_csv('raw_SSW.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = hek.HEKClient()\n",
    "start_time = pd.to_datetime('2010/06/01 00:00')\n",
    "end_time = pd.to_datetime('2016/06/01 00:00')\n",
    "ef_events = client.query(hek.attrs.Time(start_time.isoformat(),\n",
    "                                                 end_time.isoformat()),\n",
    "                                  hek.attrs.EventType('EF'))\n",
    "ef = pd.DataFrame(ef_events)\n",
    "ef.to_csv('raw_ef.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ar_events = client.query(hek.attrs.Time(start_time.isoformat(),\n",
    "                                                 end_time.isoformat()),\n",
    "                                  hek.attrs.EventType('AR'))\n",
    "ar = pd.DataFrame(ar_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cme_events = client.query(hek.attrs.Time(start_time.isoformat(),\n",
    "                                                 end_time.isoformat()),\n",
    "                                  hek.attrs.EventType('CE'))\n",
    "cme = pd.DataFrame(cme_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sg_events = client.query(hek.attrs.Time(start_time.isoformat(),\n",
    "                                                 end_time.isoformat()),\n",
    "                                  hek.attrs.EventType('SG'))\n",
    "sg = pd.DataFrame(sg_events)\n",
    "sg.to_csv('raw_sg.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fe_events = client.query(hek.attrs.Time(start_time.isoformat(),\n",
    "                                                 end_time.isoformat()),\n",
    "                                  hek.attrs.EventType('FE'))\n",
    "fe = pd.DataFrame(fe_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "er_events = client.query(hek.attrs.Time(start_time.isoformat(),\n",
    "                                                 end_time.isoformat()),\n",
    "                                  hek.attrs.EventType('ER'))\n",
    "er = pd.DataFrame(er_events)\n",
    "er.to_csv('raw_er.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fl_detective = client.query(hek.attrs.Time(start_time.isoformat(),\n",
    "#                                                  end_time.isoformat()),\n",
    "#                                   hek.attrs.EventType('FL'),\n",
    "#                                   hek.attrs.FRM.Name == 'Flare Detective - Trigger Module')\n",
    "# fl_SSW= client.query(hek.attrs.Time(start_time.isoformat(),\n",
    "#                                                  end_time.isoformat()),\n",
    "#                                   hek.attrs.EventType('FL'),\n",
    "#                                   hek.attrs.FRM.Name == 'SSW Latest Events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#may at some point add location of other flares in as well as peak flux\n",
    "def determine_wavelengths(df, index, index2 = None):\n",
    "    if index2 == None:\n",
    "        if df['obs_channelid'].values[index] == '131':\n",
    "            df['is_131'].values[index] = 1\n",
    "            df['fl_peakflux_131'].values[index] = df['fl_peakflux'].values[index]\n",
    "            \n",
    "        if df['obs_channelid'].values[index] == '171':\n",
    "            df['is_171'].values[index] = 1\n",
    "            df['fl_peakflux_171'].values[index] = df['fl_peakflux'].values[index]\n",
    "        \n",
    "        if df['obs_channelid'].values[index] == '193':\n",
    "            df['is_193'].values[index] = 1\n",
    "            df['fl_peakflux_193'].values[index] = df['fl_peakflux'].values[index]\n",
    "        \n",
    "        if df['obs_channelid'].values[index] == '211':\n",
    "            df['is_211'].values[index] = 1\n",
    "            df['fl_peakflux_211'].values[index] = df['fl_peakflux'].values[index]\n",
    "        \n",
    "        if df['obs_channelid'].values[index] == '304':\n",
    "            df['is_304'].values[index] = 1\n",
    "            df['fl_peakflux_304'].values[index] = df['fl_peakflux'].values[index]\n",
    "        \n",
    "        if df['obs_channelid'].values[index] == '355':\n",
    "            df['is_355'].values[index] = 1\n",
    "            df['fl_peakflux_355'].values[index] = df['fl_peakflux'].values[index]\n",
    "        \n",
    "        if df['obs_channelid'].values[index] == '94':\n",
    "            df['is_94'].values[index] = 1\n",
    "            df['fl_peakflux_94'].values[index] = df['fl_peakflux'].values[index]\n",
    "    else:\n",
    "        if df['obs_channelid'].values[index2] == '131':\n",
    "            df['is_131'].values[index] = 1\n",
    "            df['fl_peakflux_131'].values[index] = df['fl_peakflux'].values[index2]\n",
    "        \n",
    "        if df['obs_channelid'].values[index2] == '171':\n",
    "            df['is_171'].values[index] = 1\n",
    "            df['fl_peakflux_171'].values[index] = df['fl_peakflux'].values[index2]\n",
    "        \n",
    "        if df['obs_channelid'].values[index2] == '193':\n",
    "            df['is_193'].values[index] = 1\n",
    "            df['fl_peakflux_193'].values[index] = df['fl_peakflux'].values[index2]\n",
    "        \n",
    "        if df['obs_channelid'].values[index2] == '211':\n",
    "            df['is_211'].values[index] = 1\n",
    "            df['fl_peakflux_211'].values[index] = df['fl_peakflux'].values[index2]\n",
    "           \n",
    "        if df['obs_channelid'].values[index2] == '304':\n",
    "            df['is_304'].values[index] = 1\n",
    "            df['fl_peakflux_304'].values[index] = df['fl_peakflux'].values[index2]\n",
    "        \n",
    "        if df['obs_channelid'].values[index2] == '355':\n",
    "            df['is_355'].values[index] = 1\n",
    "            df['fl_peakflux_355'].values[index] = df['fl_peakflux'].values[index2]\n",
    "        \n",
    "        if df['obs_channelid'].values[index2] == '94':\n",
    "            df['is_94'].values[index] = 1\n",
    "            df['fl_peakflux_94'].values[index] = df['fl_peakflux'].values[index2]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fl_duplicates_detective(fl_detective, delta_t_max, distance_max, search_index):\n",
    "    delete_index = []\n",
    "    num_del = 0\n",
    "    num_duplicates = 0\n",
    "    i = 0\n",
    "    associated_fl = []\n",
    "    detective_start = map(parse_time, fl_detective['event_starttime'])\n",
    "    detective_position = map(lambda x: wkt.loads(x), fl_detective['hpc_coord'])\n",
    "    delta_t_max = datetime.timedelta(minutes = delta_t_max)\n",
    "    num_events = fl_detective.shape[0]\n",
    "    \n",
    "    #create columns in dataframe for recording multiple wavelength components of flares\n",
    "    zero_list = [0 for n in range(num_events)]\n",
    "    is_associated_fl = zero_list\n",
    "    \n",
    "    fl_detective.loc[:, 'is_131'] = zero_list\n",
    "    fl_detective.loc[:, 'is_171'] = zero_list\n",
    "    fl_detective.loc[:, 'is_193'] = zero_list\n",
    "    fl_detective.loc[:, 'is_211'] = zero_list\n",
    "    fl_detective.loc[:, 'is_304'] = zero_list\n",
    "    fl_detective.loc[:, 'is_355'] = zero_list\n",
    "    fl_detective.loc[:, 'is_355'] = zero_list\n",
    "    fl_detective.loc[:, 'is_94'] = zero_list\n",
    "    fl_detective.loc[:, 'fl_peakflux_131'] = zero_list\n",
    "    fl_detective.loc[:, 'fl_peakflux_171'] = zero_list\n",
    "    fl_detective.loc[:, 'fl_peakflux_193'] = zero_list\n",
    "    fl_detective.loc[:, 'fl_peakflux_211'] = zero_list\n",
    "    fl_detective.loc[:, 'fl_peakflux_304'] = zero_list\n",
    "    fl_detective.loc[:, 'fl_peakflux_355'] = zero_list\n",
    "    fl_detective.loc[:, 'fl_peakflux_94'] = zero_list\n",
    "    for elem in detective_start:\n",
    "        fl_match = []\n",
    "        determine_wavelengths(fl_detective, i)\n",
    "        #test whether next entry is redundant\n",
    "        if i < num_events-1:\n",
    "            if fl_detective['obs_channelid'].values[i] == fl_detective['obs_channelid'].values[i+1]:\n",
    "                if detective_start[i+1]-elem <= delta_t_max:\n",
    "                    if detective_position[i].distance(detective_position[i+1]) <= distance_max:\n",
    "                        delete_index.append(i+1)\n",
    "                        fl_match.append(fl_detective['SOL_standard'].values[i+1])\n",
    "                        num_del+=1\n",
    "        #test whether there are entries in diff wavelengths for the same flare\n",
    "        for j in range(1, search_index):\n",
    "            i2 = i+j\n",
    "            if i2 < (num_events-1):\n",
    "                if fl_detective['obs_channelid'].values[i] != fl_detective['obs_channelid'].values[i2]:\n",
    "                    if detective_start[i2]-elem <= delta_t_max:\n",
    "                        if detective_position[i].distance(detective_position[i2]) <= distance_max:\n",
    "                            delete_index.append(i2)\n",
    "                            num_duplicates+=1\n",
    "                            fl_match.append(str(fl_detective['SOL_standard'].values[i2]))\n",
    "                            determine_wavelengths(fl_detective, i, i2)\n",
    "        if fl_match == []: \n",
    "             fl_match = [0]\n",
    "        associated_fl.append(fl_match)\n",
    "        i+=1\n",
    "    fl_detective.loc[:, 'associated_fl'] = associated_fl\n",
    "    k = 0\n",
    "    for elem in associated_fl:\n",
    "        if elem!=[0]:\n",
    "            is_associated_fl[k] = 1\n",
    "        k+=1\n",
    "    delete_index = set(delete_index)\n",
    "    fl_detective.loc[:, 'is_associated_fl'] = is_associated_fl\n",
    "    fl_detective = fl_detective.drop(delete_index) \n",
    "    \n",
    "    print 'original number of events:  %d' % num_events\n",
    "    print 'duplicated events deleted:  %d' % num_del\n",
    "    print 'duplicated events merged:  %d' % (len(delete_index)-num_del)\n",
    "    print 'new number of events:  %d' % (num_events-len(delete_index))\n",
    "\n",
    "    return fl_detective     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fl_duplicates_detective2(fl_detective, delta_t_max, distance_max, search_index):\n",
    "    delete_index = []\n",
    "    delete_index2 = []\n",
    "    num_del = 0\n",
    "    num_duplicates = 0\n",
    "    i = 0\n",
    "    associated_fl = []\n",
    "    detective_start = map(parse_time, fl_detective['event_starttime'])\n",
    "    detective_position = map(lambda x: wkt.loads(x), fl_detective['hpc_coord'])\n",
    "    delta_t_max = datetime.timedelta(minutes = delta_t_max)\n",
    "    num_events = fl_detective.shape[0]\n",
    "    \n",
    "    #create columns in dataframe for recording multiple wavelength components of flares\n",
    "    zero_list = [0 for n in range(num_events)]\n",
    "    is_associated_fl = zero_list\n",
    "    idx = fl_detective['event_starttime'].index.tolist()\n",
    "    \n",
    "    fl_detective.loc[:, 'is_131'] = zero_list\n",
    "    fl_detective.loc[:, 'is_171'] = zero_list\n",
    "    fl_detective.loc[:, 'is_193'] = zero_list\n",
    "    fl_detective.loc[:, 'is_211'] = zero_list\n",
    "    fl_detective.loc[:, 'is_304'] = zero_list\n",
    "    fl_detective.loc[:, 'is_335'] = zero_list\n",
    "    fl_detective.loc[:, 'is_94'] = zero_list\n",
    "    fl_detective.loc[:, 'fl_peakflux_131'] = zero_list\n",
    "    fl_detective.loc[:, 'fl_peakflux_171'] = zero_list\n",
    "    fl_detective.loc[:, 'fl_peakflux_193'] = zero_list\n",
    "    fl_detective.loc[:, 'fl_peakflux_211'] = zero_list\n",
    "    fl_detective.loc[:, 'fl_peakflux_304'] = zero_list\n",
    "    fl_detective.loc[:, 'fl_peakflux_335'] = zero_list\n",
    "    fl_detective.loc[:, 'fl_peakflux_94'] = zero_list\n",
    "    for elem in detective_start:\n",
    "        fl_match = []\n",
    "        fl_detective = determine_wavelengths(fl_detective, i)\n",
    "        #test whether next entry is redundant\n",
    "        if i < num_events-1:\n",
    "            if fl_detective['obs_channelid'].values[i] == fl_detective['obs_channelid'].values[i+1]:\n",
    "                if detective_start[i+1]-elem <= delta_t_max:\n",
    "                    if detective_position[i].distance(detective_position[i+1]) <= distance_max:\n",
    "                        delete_index2.append(idx[i+1])\n",
    "                        fl_match.append(fl_detective['SOL_standard'].values[i+1])\n",
    "                        #fl_detective = fl_detective.drop(idx[i+1])\n",
    "                        \n",
    "                        num_del+=1\n",
    "        #test whether there are entries in diff wavelengths for the same flare\n",
    "        for j in range(1, search_index):\n",
    "            i2 = i+j\n",
    "            if i2 < (num_events-1):\n",
    "                if fl_detective['obs_channelid'].values[i] != fl_detective['obs_channelid'].values[i2]:\n",
    "                    if detective_start[i2]-elem <= delta_t_max:\n",
    "                        if detective_position[i].distance(detective_position[i2]) <= distance_max:\n",
    "                            delete_index2.append(idx[i2])\n",
    "                            fl_match.append(str(fl_detective['SOL_standard'].values[i2]))\n",
    "                            fl_detective = determine_wavelengths(fl_detective, i, i2)\n",
    "                            #fl_detective = fl_detective.drop((idx[i2]))\n",
    "                            num_duplicates+=1\n",
    "                            \n",
    "                            \n",
    "        if fl_match == []: \n",
    "             fl_match = [0]\n",
    "        associated_fl.append(fl_match)\n",
    "        i+=1\n",
    "    fl_detective.loc[:, 'associated_fl'] = associated_fl\n",
    "    k = 0\n",
    "    for elem in associated_fl:\n",
    "        if elem!=[0]:\n",
    "            is_associated_fl[k] = 1\n",
    "        k+=1\n",
    "    delete_index = set(delete_index)\n",
    "    delete_index2 = set(delete_index2)\n",
    "    #print delete_index ==delete_index2\n",
    "    fl_detective.loc[:, 'is_associated_fl'] = is_associated_fl\n",
    "    fl_detective = fl_detective.drop(delete_index2) \n",
    "    \n",
    "    print 'original number of events:  %d' % num_events\n",
    "    print 'duplicated events deleted:  %d' % num_del\n",
    "    print 'duplicated events merged:  %d' % (len(delete_index2)-num_del)\n",
    "    print 'new number of events:  %d' % (num_events-len(delete_index2))\n",
    "\n",
    "    return fl_detective     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original number of events:  50\n",
      "duplicated events deleted:  0\n",
      "duplicated events merged:  18\n",
      "new number of events:  32\n"
     ]
    }
   ],
   "source": [
    "fl_set = pd.read_csv('flare_tester.csv', delimiter = ',', header = 0)\n",
    "fl_detective_clean = fl_duplicates_detective2(fl_set, 30, 60, 10)\n",
    "fl_detective_clean = add_dist_frm_center_column(fl_detective_clean)\n",
    "flare_keywords = list(np.genfromtxt('keywords_flare.csv', delimiter=',', dtype=str))\n",
    "flare_keywords.extend(['is_associated_fl', 'associated_fl', 'is_131', 'is_171', 'is_193', 'is_211', 'is_304', \n",
    "                          'is_335','is_94', 'fl_peakflux_131', 'fl_peakflux_171','fl_peakflux_193', 'fl_peakflux_211',\n",
    "                          'fl_peakflux_304', 'fl_peakflux_335', 'fl_peakflux_94', 'dist_frm_center'])\n",
    "fl_detective_clean.to_csv('flare_tester_clean.csv', index = False, columns = flare_keywords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fl_detective = pd.DataFrame(fl_detective)\n",
    "fl_detective_clean = fl_duplicates_detective(fl_detective, 20, 60, 10)\n",
    "fl_detective_clean = add_dist_frm_center_column(fl_detective_clean)\n",
    "flare_keywords = list(np.genfromtxt('keywords_flare.csv', delimiter=',', dtype=str))\n",
    "flare_keywords.extend(['is_associated_fl', 'associated_fl', 'is_131', 'is_171', 'is_193', 'is_211', 'is_304', \n",
    "                          'is_335','is_94', 'fl_peakflux_131', 'fl_peakflux_171','fl_peakflux_193', 'fl_peakflux_211',\n",
    "                          'fl_peakflux_304', 'fl_peakflux_335', 'fl_peakflux_94', 'dist_frm_center'])\n",
    "fl_detective_clean.to_csv('flares_database.csv', index = False, columns = flare_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# delta_t_max = 10 #[minutes]\n",
    "# distance_max = 5 #[arcsec, HPC]\n",
    "# keywords_SSW = ['SOL_standard', 'event_starttime', 'event_endtime', 'event_peaktime', 'hpc_coord', \n",
    "#                 'hpc_bbox', 'fl_goescls']\n",
    "\n",
    "def fl_duplicates_SSW(fl_SSW, delta_t_max, distance_max):\n",
    "    delete_index = []\n",
    "    i = 0\n",
    "    SSW_start = fl_SSW['event_starttime']\n",
    "    SSW_start = map(parse_time, SSW_start)\n",
    "    SSW_position = map(lambda x: wkt.loads(x), fl_SSW['hpc_coord'])\n",
    "    delta_t_max = datetime.timedelta(minutes = delta_t_max)\n",
    "    \n",
    "    for elem in SSW_start:\n",
    "        j = 1\n",
    "        is_duplicate = True\n",
    "        while is_duplicate == True and i+j < len(SSW_start):\n",
    "            is_duplicate = False\n",
    "            if fl_SSW['fl_goescls'].values[i] == fl_SSW['fl_goescls'].values[i+j]:\n",
    "                if SSW_start[i+j]-elem <= delta_t_max:\n",
    "                    if SSW_position[i].distance(SSW_position[i+j]) <= distance_max:\n",
    "                        delete_index.append(i+j)\n",
    "                        is_duplicate = True\n",
    "                        j+=1\n",
    "        i+=1\n",
    "    fl_SSW = fl_SSW.drop(delete_index)\n",
    "    print delete_index\n",
    "    print fl_SSW.shape\n",
    "        \n",
    "    return fl_SSW     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fl_duplicates_SSW1(fl_SSW):\n",
    "    delete_index = []\n",
    "    i = 0\n",
    "    end = map(parse_time, fl_SSW['event_endtime'])\n",
    "    \n",
    "    for i in range(len(fl_SSW['SOL_standard'])-1):\n",
    "        j = i + 1\n",
    "        if fl_SSW['SOL_standard'].values[i] == fl_SSW['SOL_standard'].values[j]:\n",
    "            if end[i] <= end[j]:\n",
    "                delete_index.append(i)\n",
    "            else: delete_index.append(j)\n",
    "    \n",
    "    fl_SSW = fl_SSW.drop(delete_index)\n",
    "    \n",
    "    print len(delete_index)\n",
    "    \n",
    "    return fl_SSW\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2723: DtypeWarning: Columns (115) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "642\n"
     ]
    }
   ],
   "source": [
    "fl_SSW = pd.read_csv('raw_SSW.csv', delimiter = ',', header = 0 )\n",
    "fl_duplicates_SSW = fl_duplicates_SSW1(fl_SSW)\n",
    "keywords_SSW = list(np.genfromtxt('keywords_SSW_flare.csv', delimiter=',', dtype=str))\n",
    "fl_duplicates_SSW.to_csv('prepped_SSW.csv', index = False, columns = keywords_SSW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "delta_t_max = 10\n",
    "distance_max = 10\n",
    "fl_SSW = fl_duplicates_SSW(fl_SSW, delta_t_max, distance_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#match flares from flare_detective with GOES class\n",
    "#locations/times don't precisely match up between SSW entries and flare_detective entries...even though they are being \n",
    "#collected from the same satellite...\n",
    "\n",
    "def fl_goes_class():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_fl_database(start_time, end_time, delta_t_max, distance_max):\n",
    "    fl_detective = client.query(hek.attrs.Time(start_time.isoformat(),end_time.isoformat()), \n",
    "                                hek.attrs.EventType('FL'), hek.attrs.FRM.Name == 'Flare Detective - Trigger Module')\n",
    "    fl_detective = pd.DataFrame(fl_detective)\n",
    "#     fl_SSW= client.query(hek.attrs.Time(start_time.isoformat(),end_time.isoformat()), hek.attrs.EventType('FL'), \n",
    "#                          hek.attrs.FRM.Name == 'SSW Latest Events')\n",
    "#     fl_SSW = pd.Dataframe(fl_SSW)\n",
    "    delta_t_max = datetime.timedelta(minutes = delta_t_max)\n",
    "    fl_detective = fl_duplicates_detective(delta_t_max, distance_max)\n",
    "#     fl_SSW = fl_duplicates_SSW(delta_t_max, distance_max)\n",
    "    \n",
    "    flare_keywords = list(np.genfromtxt('keywords_flare.csv', delimiter=',', dtype=str))\n",
    "    flare_keywords.extend(['is_associated_fl', 'associated_fl', 'is_131', 'is_171', 'is_193', 'is_211', 'is_304', \n",
    "                          'is_335','is_94', 'fl_peakflux_131', 'fl_peakflux_171','fl_peakflux_193', 'fl_peakflux_211',\n",
    "                          'fl_peakflux_304', 'fl_peakflux_335', 'fl_peakflux_94', 'fl_goescls'])\n",
    "    fl_detective.to_csv('flares_database.csv', index = False, columns = flare_keywords)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_dist_frm_center(x, y):\n",
    "    r = 966\n",
    "    z_sq = (r**2 - x**2 - y**2)\n",
    "    if z_sq >= 0: z = z_sq**(0.5)\n",
    "    else: z = (-z_sq)**(0.5)\n",
    "    dist = (x**2 + y**2 + (z-r)**2)**(0.5)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_dist_frm_center_column(df):\n",
    "    dist_frm_center = []\n",
    "    for i in range(df.shape[0]):\n",
    "        dist = find_dist_frm_center(df['hpc_x'].values[i], df['hpc_y'].values[i])\n",
    "        dist_frm_center.append(dist)\n",
    "    df.loc[:, 'dist_frm_center'] = dist_frm_center\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TEST\n",
    "\n",
    "fl_detective = client.query(hek.attrs.Time(start_time.isoformat(),end_time.isoformat()), \n",
    "                                hek.attrs.EventType('FL'), hek.attrs.FRM.Name == 'Flare Detective - Trigger Module')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#export from HEK SSW, SWPC, & flare detective events separately\n",
    "#assume dupilcates have already been merged together\n",
    "def combine_flares_SSW_SWPC():\n",
    "    for elem in fl_SSW['event_starttime']\n",
    "    fl_SWPC\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flare_keywords = list(np.genfromtxt('flare_keywords.csv', delimiter=',', dtype=str))\n",
    "flare_keywords.append('is_associated_fl', 'associated_fl', 'is_131', 'is_171', 'is_193', 'is_211', 'is_304', 'is_335',\n",
    "                     'is_94', 'fl_peakflux_131', 'fl_peakflux_171','fl_peakflux_193', 'fl_peakflux_211',\n",
    "                     'fl_peakflux_304', 'fl_peakflux_335', 'fl_peakflux_94', 'fl_goescls')\n",
    "ef_keywords = list(np.genfromtxt('ef_keywords.csv', delimiter=',', dtype =str))\n",
    "# print flare_keywords\n",
    "# print ef_keywords\n",
    "#to change ordering of columns/delete excess ones: df = df[list_keywords]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flare.to_csv('look_at_flare_data.csv')\n",
    "# ef.to_csv('look_at_ef_data.csv')\n",
    "# ar.to_csv('look_at_ar_data.csv')\n",
    "# cme.to_csv('look_at_cme_data.csv')\n",
    "# sg.to_csv('look_at_sg_data.csv')\n",
    "# fe.to_csv('look_at_fe_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#final step to export dataframe to csv\n",
    "\n",
    "# fl.to_csv('flares_dataset.csv', index = False, columns = keywords_flare)\n",
    "# ef.to_csv('ef_dataset.csv', index = False, columns = keywords_ef)\n",
    "# cme.to_csv('cme_dataset.csv', index = False, columns = keywords_cme)\n",
    "# ar.to_csv('ar_dataset.csv', index = False, columns = keywords_ar)\n",
    "# sg.to_csv('sg_dataset.csv', index = False, columns = keywords_sigmoid)\n",
    "# fe.to_csv('fe_dataset.csv', index = False, columns = keywords_filament_eruption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "client = hek.HEKClient()\n",
    "start_time = pd.to_datetime('2010/06/11 00:00')\n",
    "end_time = pd.to_datetime('2013/01/01 00:00')\n",
    "\n",
    "ar_events1 = client.query(hek.attrs.Time(start_time.isoformat(),\n",
    "                                                 end_time.isoformat()),\n",
    "                                  hek.attrs.EventType('AR'), hek.attrs.FRM.Name!='SPoCA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time = pd.to_datetime('2013/01/01 00:00')\n",
    "end_time = pd.to_datetime('2016/06/01 00:00')\n",
    "\n",
    "ar_events2 = client.query(hek.attrs.Time(start_time.isoformat(),\n",
    "                                                 end_time.isoformat()),\n",
    "                                  hek.attrs.EventType('AR'), hek.attrs.FRM.Name!='SPoCA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def collect_ar_noaa(df, folder):\n",
    "    unique_noaa = df.ar_noaanum.unique()\n",
    "    keywords_ar = list(np.genfromtxt('keywords_ar.csv', delimiter=',', dtype =str))\n",
    "    noaa_dic = {elem : pd.DataFrame for elem in unique_noaa}\n",
    "    start_times = []\n",
    "    end_times= []\n",
    "    file_names = []\n",
    "    i = 0\n",
    "    for key in noaa_dic.keys():\n",
    "        if i != 0:\n",
    "            noaa_dic[key] = df[:][df.ar_noaanum == key]\n",
    "            start_time = noaa_dic[key]['event_starttime'].values[0]\n",
    "            end_time = noaa_dic[key]['event_endtime'].values[-1]\n",
    "            start_times.append(start_time)\n",
    "            end_times.append(end_time)\n",
    "            fName = str(key)[:-2] +'_'+ start_time[0:10]+'_'+end_time[0:10]\n",
    "            location = folder + '/' + fName+ '.csv'\n",
    "            noaa_dic[key].to_csv(location, columns = keywords_ar, index = False)\n",
    "            file_names.append(fName)\n",
    "        i+=1  \n",
    "#     file_names = np.array(file_names).T\n",
    "#     with open(folder+'/file_names.csv','w') as f:\n",
    "#         np.savetxt(f, X =file_names, fmt = '%s', delimiter=',')\n",
    " \n",
    "\n",
    "#, hek.attrs.AR.NOAANum != None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ar_keywords = np.genfromtxt('keywords_ar_1.csv', delimiter = ',', dtype = str)\n",
    "\n",
    "def merge_ar_sources(inputFile, ar_keywords):\n",
    "    ar_set =pd.read_csv(inputFile, delimiter = ',', header = 0)\n",
    "    ar_set_NOAA = ar_set.ix[ar_set['frm_identifier']=='NOAA SWPC']\n",
    "    listOindices = ar_set_NOAA.index.tolist()\n",
    "    keywords_ar_add = ['ar_mcintoshcls', 'ar_mtwilsoncls', 'ar_numspots']\n",
    "    ar_set.sort_values(by=['event_starttime','frm_identifier'], ascending =[False, True])\n",
    "    if listOindices!= [] and listOindices!=range(0, ar_set.shape[0]):\n",
    "        for i in range(0, listOindices[0]):\n",
    "            for elem in keywords_ar_add:\n",
    "                ar_set.loc[i, elem] = ar_set.loc[listOindices[0], elem] \n",
    "        for i in range(listOindices[-1], ar_set.shape[0]):\n",
    "            for elem in keywords_ar_add:\n",
    "                ar_set.loc[i, elem] = ar_set.loc[listOindices[-1], elem]\n",
    "        for i, idx in enumerate(listOindices):\n",
    "            if idx!=listOindices[-1]:\n",
    "                for j in range(idx, listOindices[i+1]):\n",
    "                    for elem in keywords_ar_add:\n",
    "                        ar_set.loc[j, elem] = ar_set.loc[idx, elem]\n",
    "        ar_set = ar_set.drop(listOindices)\n",
    "        ar_set.to_csv(inputFile, index = False, columns = ar_keywords)\n",
    "        \n",
    "    return ar_set\n",
    "\n",
    "#merge_ar_sources('11940_2014-01-01_2014-01-04.csv', ar_keywords)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ar_data1 = pd.DataFrame(ar_events1)\n",
    "collect_ar_noaa(ar_data1, 'AR3')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ar_data2 = pd.DataFrame(ar_events2)\n",
    "collect_ar_noaa(ar_data2, 'AR3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "ar_keywords = np.genfromtxt('keywords_ar_1.csv', delimiter = ',', dtype = str)\n",
    "\n",
    "for root, dirs, files in os.walk('AR1'):\n",
    "    for f in files:\n",
    "        if f[0] != '.':\n",
    "            fname = 'AR1/'+f\n",
    "            _ = merge_ar_sources(fname, ar_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "       \n",
    "                         \n",
    "                       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
