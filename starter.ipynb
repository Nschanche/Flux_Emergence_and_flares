{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#working on writing useful functions to translate/examine HEK data output\n",
    "\n",
    "import numpy as np\n",
    "from shapely.geometry import Polygon\n",
    "from shapely import wkt\n",
    "from sunpy.time import parse_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#HEK exports certain data as unicode strings. Convert these strings to relevant, useful datatypes\n",
    "\n",
    "def unicode2datetime(time_array):\n",
    "    for i, elem in time_array:\n",
    "        time_array[i] = parse_time(elem)\n",
    "        \n",
    "def unicode2polygon(bbox_array):\n",
    "    if isinstance(bbox_array, basestring):\n",
    "        bbox_array = wkt.loads(bbox_array)\n",
    "    else:\n",
    "        bbox_array = map(lambda x: wkt.loads(x), bbox_array)\n",
    "#         for i, elem in bbox_array:\n",
    "#             bbox_array[i] = wkt.loads(elem)\n",
    "    return bbox_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to determine whether 2 polygons overlap and, if so, in what manner they overlap.\n",
    "#returns a list of booleans indicating different kinds of overlap \n",
    "#input parameters must both be shapely Polygons\n",
    "\n",
    "def overlap(polygon1, polygon2):\n",
    "    is_intersect = polygon1.intersects(polygon2)\n",
    "    if is_intersect == False:\n",
    "        is_crosses = False\n",
    "        is_touches = False\n",
    "        is_contained = False\n",
    "        is_within = False\n",
    "    else:\n",
    "        is_crosses = polygon1.crosses(polygon2)\n",
    "        is_touches = polygon1.touches(polygon2)\n",
    "        is_contained = polygon1.contained(polygon2)\n",
    "        is_within = polygon1.within(polygon2)\n",
    "    return [is_intersect, is_crosses, is_touches, is_contained, is_within]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to determine the shortest distance between two preselected points on a sphere where we know \n",
    "#the radius of the sphere\n",
    "#would be of use in calculating the distance between the centers of the flux emergence and flare\n",
    "#nts: maybe change x,y inputs to be tuples later\n",
    "def geodesic(centroid_1, centroid_2, r):\n",
    "    x1 = centroid_1.x\n",
    "    x2 = centroid_2.x\n",
    "    y1 = centroid_1.y\n",
    "    y2 = centroid_2.y\n",
    "    chord = np.sqrt((x2-x1)**2 + (y2-y1)**2)\n",
    "    s = r*np.arcsin(chord/(2*r))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#assign labels to events\n",
    "#will have to determine how to access num without directly inputting\n",
    "#keep a global var? access \n",
    "def assign_label(event_type, num):\n",
    "    label = event_type + str(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-4267f98e1ba1>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-4267f98e1ba1>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    def short_dis_areas(polygon1_area, polygon2_area) #r? will have to add a column for flare area as that's not a given\u001b[0m\n\u001b[0m                                                                                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#function to find shortest distance separating two areas\n",
    "def short_dis_areas(polygon1_area, polygon2_area): #r? will have to add a column for flare area as that's not a given\n",
    "#account for normalization by looking at disk center?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function removes/sythesizes dupiclate entries for the same event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this function creates a sample data set from our merged \"database\" of flare events given parameters of start/stop \n",
    "#time, radial distance from center, and a minimum cutoff strength\n",
    "#will have to translate flare strength classifications to numbers/something sortable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import csv\n",
    "\n",
    "def pull_sample_flare(begin_year, begin_month, begin_day, end_year, end_month, end_day, max_dist_frm_center, min_strength):\n",
    "    inputFile = 'flares_database.csv'\n",
    "    flare_set = pd.read_csv(inputFile, delimiter = ',', header = 0)\n",
    "    flare_set = flare_set.ix[flare_set['start_year']>=begin_year]\n",
    "    flare_set = flare_set.ix[flare_set['stop_year']<=end_year]\n",
    "    flare_set = flare_set.ix[flare_set['start_month']>=begin_month]\n",
    "    flare_set = flare_set.ix[flare_set['stop_month']<=end_month]\n",
    "    flare_set = flare_set.ix[flare_set['start_day']>=begin_day]\n",
    "    flare_set = flare_set.ix[flare_set['stop_day']<=end_day]\n",
    "    flare_set = flare_set.ix[flare_set['dist_frm_center']<=max_dist_frm_center]\n",
    "    flare_set = flare_set.ix[flare_set['fl_peakflux']<=min_strength]\n",
    "    return flare_set\n",
    "\n",
    "# def pull_sample_flare_1(begin_time, end_time, max_dist_frm_center, min_strength):\n",
    "def pull_sample_flare_1(begin_time, end_time, min_strength):\n",
    "    inputFile = 'look_at_flare_data.csv'\n",
    "    flare_set = pd.read_csv(inputFile, delimiter = ',', header = 0)\n",
    "    flare_set['event_starttime'] = map(parse_time, flare_set['event_starttime'])\n",
    "    flare_set['event_endtime'] = map(parse_time, flare_set['event_endtime'])\n",
    "    flare_set = flare_set.ix[flare_set['event_starttime']>=begin_time]\n",
    "    flare_set = flare_set.ix[flare_set['event_endtime']<=end_time]\n",
    "    #flare_set = flare_set.ix[flare_set['dist_frm_center']<=max_dist_frm_center]\n",
    "    flare_set = flare_set.ix[flare_set['fl_peakflux']>=min_strength]\n",
    "    return flare_set\n",
    "\n",
    "begin_time = parse_time('2014-01-01T02:34:30')\n",
    "end_time = parse_time('2014-01-21T02:34:30')\n",
    "max_d = 100.0\n",
    "min_strength = 100.0\n",
    "\n",
    "print pull_sample_flare_1(begin_time, end_time, min_strength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pull_sample_ef(begin_time, end_time, delta_t):\n",
    "    inputFile = 'look_at_ef_data.csv'\n",
    "    ef_set = pd.read_csv(inputFile, delimiter = ',', header = 0)\n",
    "    ef_set['event_starttime'] = map(parse_time, ef_set['event_starttime'])\n",
    "    ef_set['event_endtime'] = map(parse_time, ef_set['event_endtime'])\n",
    "    ef_set = ef_set.ix[ef_set['event_starttime']>=(begin_time-delta_t)]\n",
    "    ef_set = ef_set.ix[ef_set['event_endtime']<=(end_time-delta_t)]\n",
    "\n",
    "    return ef_set\n",
    "begin_time = parse_time('2014-01-01T02:34:30')\n",
    "end_time = parse_time('2014-01-21T02:34:30')\n",
    "delta_t=datetime.timedelta(hours = 0, minutes = 30, seconds = 0)\n",
    "print pull_sample_ef(begin_time, end_time, delta_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spatial_flux_check(flare_poly, ef_poly, spatial_sep, df, index):\n",
    "    if geodesic(flare_poly.centroid, ef_poly.centroid)<= spatial_sep:\n",
    "        return df['indentifier'][index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to determine whether a flare corresponds with a flux emergence\n",
    "\n",
    "def ff(begin_year, begin_month, begin_day, end_year, end_month, end_day, max_dist_frm_center, min_strength, \n",
    "       temporal_sep_hr, temporal_sep_min, temporal_sep_sec, spatial_sep, output2file=False, out_file = None):\n",
    "    delta_t = datetime.timedelta(hours = temporal_sep_hr, minutes = temporal_sep_min, seconds = temporal_sep_sec)\n",
    "    begin_time = datetime.datetime(begin_year, begin_month, begin_day)\n",
    "    end_time = datetime.datetime(end_year, end_month, end_day) \n",
    "    #flare_set = pull_sample_flare_1(begin_time, end_time, max_dist_frm_center, min_strength)\n",
    "    flare_set = pull_sample_flare_1(begin_time, end_time, min_strength)\n",
    "    ef_set = pull_sample_ef(begin_time, end_time, delta_t)\n",
    "    \n",
    "    is_ef = [0 for i in range(flare_set.shape[0])]\n",
    "    associated_ef = []\n",
    "    i = -1\n",
    "    for elem in flare_set['event_starttime']:\n",
    "        i += 1\n",
    "        start_ef = elem-delta_t\n",
    "        ef_search = ef_set.ix[ef_set['event_endtime']>=start_ef]\n",
    "        ef_search = ef_search.ix[ef_search['event_endtime']<=elem]\n",
    "        if ef_search.empty == False:\n",
    "            fl_hpc_bbox = np.array(flare_set['hpc_bbox'])\n",
    "            flare_poly = wkt.loads(fl_hpc_bbox[i])\n",
    "            good_ef = []\n",
    "            if ef_search.shape[0] == 1:\n",
    "                ef_poly = wkt.loads(ef_search['hpc_bbox'].values[0])\n",
    "                if geodesic(flare_poly.centroid, ef_poly.centroid, 966)<= spatial_sep:\n",
    "                    good_ef.append(ef_search['SOL_standard'].values[0])\n",
    "                else: good_ef = [0]\n",
    "            else:\n",
    "                ef_search['hpc_bbox'] = unicode2polygon(ef_search['hpc_bbox'])\n",
    "                j = 0\n",
    "                for elem in ef_search['hpc_bbox'].values:\n",
    "                    if geodesic(flare_poly.centroid, elem.centroid, 966)<= spatial_sep:\n",
    "                        good_ef.append(ef_search['SOL_standard'].values[j])\n",
    "                    j+=1\n",
    "                else: good_ef = [0]\n",
    "        else: good_ef = [0]\n",
    "        associated_ef.append(good_ef)\n",
    "    k = 0\n",
    "    for elem in associated_ef:\n",
    "        if elem!=[0]:\n",
    "            is_ef[k] = 1\n",
    "        k+=1\n",
    "    flare_set.loc[:, 'is_ef'] = is_ef\n",
    "    flare_set.loc[:, 'associated_ef'] = associated_ef\n",
    "    \n",
    "    if output2file == True:\n",
    "        if out_file == None:\n",
    "            out_file = ('flare_search'+str(begin_time)+'_'+ str(end_time)+'_'+str(min_strength)+'.csv')\n",
    "#             out_file = ('flare_search_'+str(begin_time)+'_'+ str(end_time)+'_'+str(max_dist_from_center)+\n",
    "#                         '_'+str(min_strength)+'.csv')\n",
    "        flare_set.to_csv(path_or_buf=out_file, index = False)\n",
    "        \n",
    "    return flare_set   \n",
    "        \n",
    "no = ff(2014, 1, 2, 2014, 1, 3, 100, 100, \n",
    "               0, 30, 0, 300, output2file=False, out_file = None)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
