{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from shapely import wkt\n",
    "from sunpy.time import parse_time\n",
    "import datetime\n",
    "from shapely.geometry import Polygon, Point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unicode2polygon(bbox_array):\n",
    "    if isinstance(bbox_array, basestring):\n",
    "        bbox_array = wkt.loads(bbox_array)\n",
    "    else:\n",
    "        bbox_array = map(lambda x: wkt.loads(x), bbox_array)\n",
    "#         for i, elem in bbox_array:\n",
    "#             bbox_array[i] = wkt.loads(elem)\n",
    "    return bbox_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#may at some point add location of other flares in as well as peak flux\n",
    "def determine_wavelengths(df, index, index2 = None):\n",
    "    if index2 == None:\n",
    "        if df['obs_channelid'].values[index] == 131:\n",
    "            df['is_131'].values[index] = 1\n",
    "            df['fl_peakflux_131'].values[index] = df['fl_peakflux'].values[index]\n",
    "            df['starttime_131'].values[index] = df['event_starttime'].values[index]\n",
    "            df['peaktime_131'].values[index] = df['event_peaktime'].values[index]\n",
    "            df['endtime_131'].values[index] = df['event_endtime'].values[index]\n",
    "            df['hpc_x_131'].values[index] = df['hpc_x'].values[index]\n",
    "            df['hpc_y_131'].values[index] = df['hpc_y'].values[index]\n",
    "            \n",
    "        if df['obs_channelid'].values[index] == 171:\n",
    "            df['is_171'].values[index] = 1\n",
    "            df['fl_peakflux_171'].values[index] = df['fl_peakflux'].values[index]\n",
    "            df['starttime_171'].values[index] = df['event_starttime'].values[index]\n",
    "            df['peaktime_171'].values[index] = df['event_peaktime'].values[index]\n",
    "            df['endtime_171'].values[index] = df['event_endtime'].values[index]\n",
    "            df['hpc_x_171'].values[index] = df['hpc_x'].values[index]\n",
    "            df['hpc_y_171'].values[index] = df['hpc_y'].values[index]\n",
    "        \n",
    "        if df['obs_channelid'].values[index] == 193:\n",
    "            df['is_193'].values[index] = 1\n",
    "            df['fl_peakflux_193'].values[index] = df['fl_peakflux'].values[index]\n",
    "            df['starttime_193'].values[index] = df['event_starttime'].values[index]\n",
    "            df['peaktime_193'].values[index] = df['event_peaktime'].values[index]\n",
    "            df['endtime_193'].values[index] = df['event_endtime'].values[index]\n",
    "            df['hpc_x_193'].values[index] = df['hpc_x'].values[index]\n",
    "            df['hpc_y_193'].values[index] = df['hpc_y'].values[index]\n",
    "        \n",
    "        if df['obs_channelid'].values[index] == 211:\n",
    "            df['is_211'].values[index] = 1\n",
    "            df['fl_peakflux_211'].values[index] = df['fl_peakflux'].values[index]\n",
    "            df['starttime_211'].values[index] = df['event_starttime'].values[index]\n",
    "            df['peaktime_211'].values[index] = df['event_peaktime'].values[index]\n",
    "            df['endtime_211'].values[index] = df['event_endtime'].values[index]\n",
    "            df['hpc_x_211'].values[index] = df['hpc_x'].values[index]\n",
    "            df['hpc_y_211'].values[index] = df['hpc_y'].values[index]\n",
    "        \n",
    "        if df['obs_channelid'].values[index] == 304:\n",
    "            df['is_304'].values[index] = 1\n",
    "            df['fl_peakflux_304'].values[index] = df['fl_peakflux'].values[index]\n",
    "            df['starttime_304'].values[index] = df['event_starttime'].values[index]\n",
    "            df['peaktime_304'].values[index] = df['event_peaktime'].values[index]\n",
    "            df['endtime_304'].values[index] = df['event_endtime'].values[index]\n",
    "            df['hpc_x_304'].values[index] = df['hpc_x'].values[index]\n",
    "            df['hpc_y_304'].values[index] = df['hpc_y'].values[index]\n",
    "        \n",
    "        if df['obs_channelid'].values[index] == 335:\n",
    "            df['is_335'].values[index] = 1\n",
    "            df['fl_peakflux_335'].values[index] = df['fl_peakflux'].values[index]\n",
    "            df['starttime_335'].values[index] = df['event_starttime'].values[index]\n",
    "            df['peaktime_335'].values[index] = df['event_peaktime'].values[index]\n",
    "            df['endtime_335'].values[index] = df['event_endtime'].values[index]\n",
    "            df['hpc_x_335'].values[index] = df['hpc_x'].values[index]\n",
    "            df['hpc_y_335'].values[index] = df['hpc_y'].values[index]\n",
    "        \n",
    "        if df['obs_channelid'].values[index] == 94:\n",
    "            df['is_94'].values[index] = 1\n",
    "            df['fl_peakflux_94'].values[index] = df['fl_peakflux'].values[index]\n",
    "            df['starttime_94'].values[index] = df['event_starttime'].values[index]\n",
    "            df['peaktime_94'].values[index] = df['event_peaktime'].values[index]\n",
    "            df['endtime_94'].values[index] = df['event_endtime'].values[index]\n",
    "            df['hpc_x_94'].values[index] = df['hpc_x'].values[index]\n",
    "            df['hpc_y_94'].values[index] = df['hpc_y'].values[index]\n",
    "    else:\n",
    "        if df['obs_channelid'].values[index2] == 131:\n",
    "            df['is_131'].values[index] = 1\n",
    "            df['fl_peakflux_131'].values[index] = df['fl_peakflux'].values[index2]\n",
    "            df['starttime_131'].values[index] = df['event_starttime'].values[index2]\n",
    "            df['peaktime_131'].values[index] = df['event_peaktime'].values[index2]\n",
    "            df['endtime_131'].values[index] = df['event_endtime'].values[index2]\n",
    "            df['hpc_x_131'].values[index] = df['hpc_x'].values[index2]\n",
    "            df['hpc_y_131'].values[index] = df['hpc_y'].values[index2]\n",
    "        \n",
    "        if df['obs_channelid'].values[index2] == 171:\n",
    "            df['is_171'].values[index] = 1\n",
    "            df['fl_peakflux_171'].values[index] = df['fl_peakflux'].values[index2]\n",
    "            df['starttime_171'].values[index] = df['event_starttime'].values[index2]\n",
    "            df['peaktime_171'].values[index] = df['event_peaktime'].values[index2]\n",
    "            df['endtime_171'].values[index] = df['event_endtime'].values[index2]\n",
    "            df['hpc_x_171'].values[index] = df['hpc_x'].values[index2]\n",
    "            df['hpc_y_171'].values[index] = df['hpc_y'].values[index2]\n",
    "        \n",
    "        if df['obs_channelid'].values[index2] == 193:\n",
    "            df['is_193'].values[index] = 1\n",
    "            df['fl_peakflux_193'].values[index] = df['fl_peakflux'].values[index2]\n",
    "            df['starttime_193'].values[index] = df['event_starttime'].values[index2]\n",
    "            df['peaktime_193'].values[index] = df['event_peaktime'].values[index2]\n",
    "            df['endtime_193'].values[index] = df['event_endtime'].values[index2]\n",
    "            df['hpc_x_193'].values[index] = df['hpc_x'].values[index2]\n",
    "            df['hpc_y_193'].values[index] = df['hpc_y'].values[index2]\n",
    "        \n",
    "        if df['obs_channelid'].values[index2] == 211:\n",
    "            df['is_211'].values[index] = 1\n",
    "            df['fl_peakflux_211'].values[index] = df['fl_peakflux'].values[index2]\n",
    "            df['starttime_211'].values[index] = df['event_starttime'].values[index2]\n",
    "            df['peaktime_211'].values[index] = df['event_peaktime'].values[index2]\n",
    "            df['endtime_211'].values[index] = df['event_endtime'].values[index2]\n",
    "            df['hpc_x_211'].values[index] = df['hpc_x'].values[index2]\n",
    "            df['hpc_y_211'].values[index] = df['hpc_y'].values[index2]\n",
    "           \n",
    "        if df['obs_channelid'].values[index2] == 304:\n",
    "            df['is_304'].values[index] = 1\n",
    "            df['fl_peakflux_304'].values[index] = df['fl_peakflux'].values[index2]\n",
    "            df['starttime_304'].values[index] = df['event_starttime'].values[index2]\n",
    "            df['peaktime_304'].values[index] = df['event_peaktime'].values[index2]\n",
    "            df['endtime_304'].values[index] = df['event_endtime'].values[index2]\n",
    "            df['hpc_x_304'].values[index] = df['hpc_x'].values[index2]\n",
    "            df['hpc_y_304'].values[index] = df['hpc_y'].values[index2]\n",
    "        \n",
    "        if df['obs_channelid'].values[index2] == 335:\n",
    "            df['is_335'].values[index] = 1\n",
    "            df['fl_peakflux_335'].values[index] = df['fl_peakflux'].values[index2]\n",
    "            df['starttime_335'].values[index] = df['event_starttime'].values[index2]\n",
    "            df['peaktime_335'].values[index] = df['event_peaktime'].values[index2]\n",
    "            df['endtime_335'].values[index] = df['event_endtime'].values[index2]\n",
    "            df['hpc_x_335'].values[index] = df['hpc_x'].values[index2]\n",
    "            df['hpc_y_335'].values[index] = df['hpc_y'].values[index2]\n",
    "        \n",
    "        if df['obs_channelid'].values[index2] == 94:\n",
    "            df['is_94'].values[index] = 1\n",
    "            df['fl_peakflux_94'].values[index] = df['fl_peakflux'].values[index2]\n",
    "            df['starttime_94'].values[index] = df['event_starttime'].values[index2]\n",
    "            df['peaktime_94'].values[index] = df['event_peaktime'].values[index2]\n",
    "            df['endtime_94'].values[index] = df['event_endtime'].values[index2]\n",
    "            df['hpc_x_94'].values[index] = df['hpc_x'].values[index2]\n",
    "            df['hpc_y_94'].values[index] = df['hpc_y'].values[index2]\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fl_duplicates_detective(fl_detective, delta_t_max, distance_max, search_index):\n",
    "    delete_index = []\n",
    "    num_del = 0\n",
    "    num_duplicates = 0\n",
    "    i = 0\n",
    "    associated_fl = []\n",
    "    detective_peak = map(parse_time, fl_detective['event_peaktime'])\n",
    "    detective_position = map(lambda x: wkt.loads(x), fl_detective['hpc_coord'])\n",
    "    delta_t_max = datetime.timedelta(minutes = delta_t_max)\n",
    "    num_events = fl_detective.shape[0]\n",
    "    \n",
    "    #create columns in dataframe for recording multiple wavelength components of flares\n",
    "    zero_list = [0 for n in range(num_events)]\n",
    "    none_list = [None for n in range(num_events)]\n",
    "    is_associated_fl = zero_list\n",
    "    keywords_flare_zeroes = list(np.genfromtxt('keywords_flare_zeroes.csv', delimiter=',', dtype=str))\n",
    "    keywords_flare_nones = list(np.genfromtxt('keywords_flare_nones.csv', delimiter=',', dtype=str))\n",
    "    for elem in keywords_flare_zeroes:\n",
    "        fl_detective.loc[:, elem] = zero_list  \n",
    "    \n",
    "    for elem in keywords_flare_nones:\n",
    "        fl_detective.loc[:, elem] = none_list\n",
    "    \n",
    "    for elem in detective_peak:\n",
    "        fl_match = []\n",
    "        fl_detective = determine_wavelengths(fl_detective, i)\n",
    "        #test whether next entry is redundant\n",
    "        if i < num_events-1:\n",
    "            if fl_detective['obs_channelid'].values[i] == fl_detective['obs_channelid'].values[i+1]:\n",
    "                if detective_peak[i+1]-elem <= delta_t_max:\n",
    "                    if detective_position[i].distance(detective_position[i+1]) <= distance_max:\n",
    "                        delete_index.append(i+1)\n",
    "                        fl_match.append(str(fl_detective['SOL_standard'].values[i+1]))\n",
    "                        num_del+=1\n",
    "        #test whether there are entries in diff wavelengths for the same flare\n",
    "        for j in range(1, search_index):\n",
    "            i2 = i+j\n",
    "            if i2 < (num_events-1):\n",
    "                if fl_detective['obs_channelid'].values[i] != fl_detective['obs_channelid'].values[i2]:\n",
    "                    if detective_peak[i2]-elem <= delta_t_max:\n",
    "                        if detective_position[i].distance(detective_position[i2]) <= distance_max:\n",
    "                            delete_index.append(i2)\n",
    "                            num_duplicates+=1\n",
    "                            fl_match.append(str(fl_detective['SOL_standard'].values[i2]))\n",
    "                            fl_detective = determine_wavelengths(fl_detective, i, i2)\n",
    "        fl_detective.loc[i, 'sum_peakflux'] = (fl_detective.loc[i, 'fl_peakflux_131']+ \n",
    "                                               fl_detective.loc[i, 'fl_peakflux_171']+\n",
    "                                               fl_detective.loc[i, 'fl_peakflux_193']+\n",
    "                                               fl_detective.loc[i, 'fl_peakflux_211']+\n",
    "                                               fl_detective.loc[i, 'fl_peakflux_304']+\n",
    "                                               fl_detective.loc[i, 'fl_peakflux_335']+\n",
    "                                               fl_detective.loc[i, 'fl_peakflux_94'])\n",
    "        if fl_match == []: \n",
    "             fl_match = None\n",
    "        associated_fl.append(fl_match)\n",
    "        i+=1\n",
    "    fl_detective.loc[:, 'associated_fl'] = associated_fl\n",
    "    k = 0\n",
    "    for elem in associated_fl:\n",
    "        if elem!=None:\n",
    "            is_associated_fl[k] = 1\n",
    "        k+=1\n",
    "    delete_index = set(delete_index)\n",
    "    fl_detective.loc[:, 'is_associated_fl'] = is_associated_fl\n",
    "    fl_detective = fl_detective.drop(delete_index) \n",
    "    \n",
    "    print 'original number of events:  %d' % num_events\n",
    "    print 'duplicated events deleted:  %d' % num_del\n",
    "    print 'duplicated events merged:  %d' % (len(delete_index)-num_del)\n",
    "    print 'new number of events:  %d' % (num_events-len(delete_index))\n",
    "    print 'actual new number of events:  %d' %fl_detective.shape[0]\n",
    "\n",
    "    return fl_detective     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original number of events:  87044\n",
      "duplicated events deleted:  4077\n",
      "duplicated events merged:  36800\n",
      "new number of events:  46167\n",
      "actual new number of events:  46167\n"
     ]
    }
   ],
   "source": [
    "flare_keywords = list(np.genfromtxt('keywords_flare.csv', delimiter=',', dtype=str))\n",
    "fl_set = pd.read_csv('raw_detective.csv', delimiter = ',', header = 0, usecols = flare_keywords)\n",
    "fl_set = fl_duplicates_detective(fl_set, 30, 50, 8)\n",
    "flare_keywords_after_merge = list(np.genfromtxt('keywords_flare_after_merge.csv', delimiter=',', dtype=str))\n",
    "fl_set.to_csv('flare_dataset_cleaned.csv', index = False, columns = flare_keywords_after_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_cols(df):\n",
    "    #add a column for distance from flare center to disk center (dist_frm_center)\n",
    "    df = add_dist_frm_center_column(df)\n",
    "    #add column for duration of flare\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#find the distance from center of disk from x, y coordinates (unit = arcsec)\n",
    "def find_dist_frm_center(x, y):\n",
    "    #set the radius of the Sun\n",
    "    r = 966\n",
    "    #calculate z squared\n",
    "    z_sq = (r**2 - x**2 - y**2)\n",
    "    #if loop to prevent imaginary numbers when taking square root of z squared\n",
    "    if z_sq >= 0: z = z_sq**(0.5)\n",
    "    else: z = (-z_sq)**(0.5)\n",
    "    #calculate the distance to center, disk center @ (0, 0, R)\n",
    "    dist = (x**2 + y**2 + (z-r)**2)**(0.5)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_dist_frm_center_column(df):\n",
    "#     dist_frm_center = []\n",
    "    dist_frm_center = map(lambda x, y: find_dist_frm_center(x, y), df['hpc_x'], df['hpc_y'])\n",
    "#     for i in range(df.shape[0]):\n",
    "#         dist = find_dist_frm_center(df['hpc_x'].values[i], df['hpc_y'].values[i])\n",
    "#         dist_frm_center.append(dist)\n",
    "    df.loc[:, 'dist_frm_center'] = dist_frm_center\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_duration(start_time, stop_time):\n",
    "    duration = ((stop_time - start_time).total_seconds())/60\n",
    "    return duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fl_duplicates_SSW1(fl_SSW):\n",
    "    delete_index = []\n",
    "    i = 0\n",
    "    end = map(parse_time, fl_SSW['event_endtime'])\n",
    "    \n",
    "    for i in range(len(fl_SSW['SOL_standard'])-1):\n",
    "        j = i + 1\n",
    "        if fl_SSW['SOL_standard'].values[i] == fl_SSW['SOL_standard'].values[j]:\n",
    "            if end[i] <= end[j]:\n",
    "                delete_index.append(i)\n",
    "            else: delete_index.append(j)\n",
    "    \n",
    "    fl_SSW = fl_SSW.drop(delete_index)\n",
    "    \n",
    "    print len(delete_index)\n",
    "    \n",
    "    return fl_SSW    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fl_SSW = pd.read_csv('raw_SSW.csv', delimiter = ',', header = 0 )\n",
    "fl_duplicates_SSW = fl_duplicates_SSW1(fl_SSW)\n",
    "keywords_SSW = list(np.genfromtxt('keywords_SSW_flare.csv', delimiter=',', dtype=str))\n",
    "fl_duplicates_SSW.to_csv('prepped_SSW.csv', index = False, columns = keywords_SSW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def associate_GOES(inputFile_fl, inputFile_goes, spatial_sep, output2file=False, out_file = None):\n",
    "    #import a record of flare events as a DataFrame\n",
    "    flare_set = pd.read_csv(inputFile_fl, delimiter = ',', header = 0)\n",
    "    #import a record of GOES events as a DataFrame\n",
    "    goes_set = pd.read_csv(inputFile_goes, delimiter = ',', header = 0)\n",
    "    #set solar radius\n",
    "    r = 966\n",
    "    #how many flare events working with\n",
    "    length = flare_set.shape[0]\n",
    "    #list of zeroes with length of number of flare events\n",
    "    zeroes = [0 for i in range(length)]\n",
    "    #list of nulls with length of number of flare events\n",
    "    nones = [None for i in range(length)]\n",
    "    #create columns (filled with zeroes) for tracking associated events\n",
    "    flare_set.loc[:, 'associated_goes'] = nones\n",
    "\n",
    "    goes_set = goes_set.rename(columns={'SOL_standard':'goes_SOL_standard','event_starttime':'starttime_goes', \n",
    "                                        'event_endtime':'endtime_goes', 'event_peaktime':'peaktime_goes',\n",
    "                                        'hpc_x':'hpc_x_goes', 'hpc_y':'hpc_y_goes', 'ar_noaanum': 'goes_noaanum',\n",
    "                                        'fl_goescls':'goescls'})\n",
    "    goes_keywords = ['starttime_goes', 'endtime_goes', 'peaktime_goes', 'hpc_x_goes', 'hpc_y_goes', 'goescls']\n",
    "   \n",
    "    #create columns filled with Nones for each GOES keyword\n",
    "    for elem in goes_keywords:\n",
    "         flare_set.loc[:, elem] = nones\n",
    "    #convert start and end times to datetime objects\n",
    "    flare_set['event_starttime'] = map(parse_time, flare_set['event_starttime'])\n",
    "    flare_set['event_endtime'] = map(parse_time, flare_set['event_endtime'])\n",
    "    goes_set['starttime_goes'] = map(parse_time, goes_set['starttime_goes'])\n",
    "    goes_set['endtime_goes'] = map(parse_time, goes_set['endtime_goes'])\n",
    "#     ar_list['start_time'] = map(lambda x, y, z: datetime.datetime(x, y, z), ar_list['start_year'], \n",
    "#                                 ar_list['start_month'], ar_list['start_day'])\n",
    "#     ar_list['end_time'] = map(lambda x, y, z: datetime.datetime(x, y, z), ar_list['end_year'], \n",
    "#                                   ar_list['end_month'], ar_list['end_day'])\n",
    "    #set positional row index \n",
    "    i = -1\n",
    "\n",
    "    for annoying_wrong_obj_type in flare_set['event_starttime']:\n",
    "        i += 1\n",
    "        #print which flare event function is currently processing, so the user has an idea of how much longer\n",
    "        #program will need to run\n",
    "        start = flare_set['event_starttime'].values[i]\n",
    "        end = flare_set['event_endtime'].values[i]\n",
    "        if (i+1)%10 == 0:\n",
    "            print '%d / %d events' %((i+1), length)\n",
    "        #begin eliminating ar events based on temporal parameters\n",
    "        goes_search = goes_set.ix[goes_set['starttime_goes']<=start]\n",
    "        goes_search = goes_search.ix[goes_search['endtime_goes']>=end]\n",
    "        \n",
    "        #as long as the temporal search does not eliminate all possible related AR events, proceed\n",
    "        if goes_search.empty==False:\n",
    "                fl_point = Point((flare_set['hpc_x'].values[i], flare_set['hpc_y'].values[i]))\n",
    "                min_s = spatial_sep\n",
    "                event_index = None\n",
    "                print 'for index %d, # elements in goes search is %d' % (i, (goes_search.shape[0]))\n",
    "                for j in range(goes_search.shape[0]):\n",
    "                    found_goes = False\n",
    "                    #create a point object from GOES flare's location\n",
    "                    goes_point = Point((goes_search['hpc_x_goes'].values[j], goes_search['hpc_y_goes'].values[j]))\n",
    "                    #calculate the minimum 2D distance between the AIA flare's and GOES flare's mean coordinates\n",
    "                    chord = fl_point.distance(goes_point)\n",
    "                    #calculate the minimum 3D distance along the sun's curved surface between the flare events\n",
    "                    #assumes the same radius for all events\n",
    "                    s = r*np.arcsin(chord/(2*r))\n",
    "                    if i==9:print found_goes\n",
    "                    #determine whether the spatial distance between AIA and GOES flares meets the set parameter\n",
    "                    if s <= spatial_sep:\n",
    "                        #have found an associated GOES flare\n",
    "                        found_goes = True\n",
    "                        if s <= min_s: \n",
    "                            print 'for flare %d, good event index %d' % (i, j)\n",
    "                            min_s = s\n",
    "                            event_index = j\n",
    "                if found_goes:\n",
    "                    print i, event_index\n",
    "                    flare_set['associated_goes'].values[i] = goes_search['goes_SOL_standard'].values[event_index]\n",
    "                    for elem in goes_keywords:\n",
    "                        flare_set.loc[i, elem] = goes_search[elem].values[event_index]\n",
    "            \n",
    "    #create boolean var to easily determine whether flare associated with an AR\n",
    "    k = 0\n",
    "    is_goes = [0 for i in range(flare_set.shape[0])]\n",
    "    for elem in flare_set['associated_goes']:\n",
    "        if elem!=None:\n",
    "            is_goes[k] = 1\n",
    "        k+=1\n",
    "    flare_set.loc[:, 'is_goes'] = is_goes\n",
    "    \n",
    "    #write dataframe to a csv file depending on initial parameters\n",
    "    if output2file:\n",
    "        if out_file == None:\n",
    "            #create a generic name for file based on search parameters if no file name specified\n",
    "             out_file = inputFile_fl[0:-4]+'_with_GOES.csv'\n",
    "        #import which keywords to keep for outported data\n",
    "        flare_keywords = list(np.genfromtxt('keywords_flare_with_ef_with_ar.csv', delimiter=',', dtype=str))\n",
    "        #add to these keywords descriptors of associated GOES flare\n",
    "        flare_keywords.extend(['is_goes','associated_goes'])\n",
    "        flare_keywords.extend(goes_keywords)\n",
    "        #write to csv\n",
    "        flare_set.to_csv(path_or_buf=out_file, columns = flare_keywords, index = False)\n",
    "        \n",
    "    return flare_set\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for index 5, # elements in goes search is 1\n",
      "for flare 5, good event index 0\n",
      "5 0\n",
      "10 / 18 events\n",
      "for index 9, # elements in goes search is 1\n",
      "False\n",
      "for index 13, # elements in goes search is 1\n",
      "for flare 13, good event index 0\n",
      "13 0\n",
      "for index 14, # elements in goes search is 1\n",
      "for flare 14, good event index 0\n",
      "14 0\n"
     ]
    }
   ],
   "source": [
    "#associate_GOES(inputFile_fl, inputFile_goes, spatial_sep, output2file=False, out_file = None)\n",
    "goes_output = associate_GOES('test_goes_detective.csv', 'test_goes.csv', 200, output2file=False, out_file = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      SOL_standard     event_starttime       event_endtime  \\\n",
      "0   SOL2010-06-11T20:27:24L105C069 2010-06-11 20:27:24 2010-06-11 21:02:48   \n",
      "1   SOL2010-06-11T20:27:26L105C069 2010-06-11 20:27:26 2010-06-12 14:31:26   \n",
      "2   SOL2010-06-12T05:20:39L100C069 2010-06-12 05:20:39 2010-06-12 05:27:03   \n",
      "3   SOL2010-06-12T09:04:03L098C069 2010-06-12 09:04:03 2010-06-12 09:29:27   \n",
      "4   SOL2010-06-13T05:35:35L128C115 2010-06-13 05:35:35 2010-06-13 05:50:47   \n",
      "5   SOL2010-06-13T05:36:44L128C111 2010-06-13 05:36:44 2010-06-13 05:42:20   \n",
      "6   SOL2010-06-13T05:54:48L105C069 2010-06-13 05:54:48 2010-06-13 05:58:24   \n",
      "7   SOL2010-06-13T07:05:36L110C116 2010-06-13 07:05:36 2010-06-13 07:16:00   \n",
      "8   SOL2010-06-13T07:33:00L097C064 2010-06-13 07:33:00 2010-06-13 07:38:24   \n",
      "9   SOL2010-06-13T07:33:15L094C069 2010-06-13 07:33:15 2010-06-13 07:37:03   \n",
      "10  SOL2010-06-13T08:09:39L104C069 2010-06-13 08:09:39 2010-06-13 08:17:51   \n",
      "11  SOL2010-06-13T09:39:26L103C069 2010-06-13 09:39:26 2010-06-13 09:56:26   \n",
      "12  SOL2010-06-13T10:51:27L102C069 2010-06-13 10:51:27 2010-06-13 11:00:51   \n",
      "13  SOL2010-06-13T13:21:50L101C069 2010-06-13 13:21:50 2010-06-13 13:24:26   \n",
      "14  SOL2010-06-13T18:12:03L098C069 2010-06-13 18:12:03 2010-06-13 18:14:03   \n",
      "15  SOL2010-06-23T11:08:27L308C063 2010-06-23 11:08:27 2010-06-23 11:12:51   \n",
      "16  SOL2010-06-27T03:11:02L203C114 2010-06-27 03:11:02 2010-06-27 08:47:14   \n",
      "17  SOL2010-06-27T03:16:48L149C110 2010-06-27 03:16:48 2010-06-27 03:20:48   \n",
      "\n",
      "         event_peaktime  fl_goescls  \\\n",
      "0   2010-06-11T20:37:36         NaN   \n",
      "1   2010-06-12T01:00:14         NaN   \n",
      "2   2010-06-12T05:21:39         NaN   \n",
      "3   2010-06-12T09:14:27         NaN   \n",
      "4   2010-06-13T05:40:59         NaN   \n",
      "5   2010-06-13T05:39:44         NaN   \n",
      "6   2010-06-13T05:56:12         NaN   \n",
      "7   2010-06-13T07:08:48         NaN   \n",
      "8   2010-06-13T07:34:48         NaN   \n",
      "9   2010-06-13T07:34:51         NaN   \n",
      "10  2010-06-13T08:13:03         NaN   \n",
      "11  2010-06-13T09:45:38         NaN   \n",
      "12  2010-06-13T10:54:15         NaN   \n",
      "13  2010-06-13T13:22:14         NaN   \n",
      "14  2010-06-13T18:13:03         NaN   \n",
      "15  2010-06-23T11:09:15         NaN   \n",
      "16  2010-06-27T03:15:26         NaN   \n",
      "17  2010-06-27T03:17:36         NaN   \n",
      "\n",
      "                                             hpc_bbox             hpc_coord  \\\n",
      "0   POLYGON((614.4 307.2,691.2 307.2,691.2 384,614...    POINT(652.8 345.6)   \n",
      "1   POLYGON((614.4 307.2,768 307.2,768 460.8,614.4...    POINT(652.8 345.6)   \n",
      "2   POLYGON((614.4 307.2,691.2 307.2,691.2 384,614...    POINT(652.8 345.6)   \n",
      "3   POLYGON((614.4 307.2,768 307.2,768 460.8,614.4...    POINT(652.8 345.6)   \n",
      "4   POLYGON((844.8 -460.8,921.6 -460.8,921.6 -307....   POINT(883.2 -422.4)   \n",
      "5   POLYGON((844.8 -384,921.6 -384,921.6 -307.2,84...   POINT(883.2 -345.6)   \n",
      "6   POLYGON((768 307.2,844.8 307.2,844.8 384,768 3...    POINT(806.4 345.6)   \n",
      "7   POLYGON((768 -460.8,844.8 -460.8,844.8 -384,76...   POINT(806.4 -422.4)   \n",
      "8   POLYGON((691.2 307.2,768 307.2,768 460.8,691.2...    POINT(729.6 422.4)   \n",
      "9   POLYGON((691.2 307.2,768 307.2,768 460.8,691.2...    POINT(729.6 345.6)   \n",
      "10  POLYGON((768 307.2,844.8 307.2,844.8 384,768 3...    POINT(806.4 345.6)   \n",
      "11  POLYGON((768 307.2,844.8 307.2,844.8 384,768 3...    POINT(806.4 345.6)   \n",
      "12  POLYGON((768 307.2,844.8 307.2,844.8 384,768 3...    POINT(806.4 345.6)   \n",
      "13  POLYGON((768 307.2,844.8 307.2,844.8 384,768 3...    POINT(806.4 345.6)   \n",
      "14  POLYGON((768 307.2,844.8 307.2,844.8 384,768 3...    POINT(806.4 345.6)   \n",
      "15  POLYGON((537.6 384,614.4 384,614.4 460.8,537.6...      POINT(576 422.4)   \n",
      "16  POLYGON((-230.4 -460.8,-153.6 -460.8,-153.6 -3...    POINT(-192 -422.4)   \n",
      "17  POLYGON((-844.8 -384,-768 -384,-768 -307.2,-84...  POINT(-806.4 -345.6)   \n",
      "\n",
      "    hpc_radius  hpc_x  hpc_y   ...     hpc_x_94 hpc_y_94  \\\n",
      "0   738.638748  652.8  345.6   ...          NaN      NaN   \n",
      "1   738.638748  652.8  345.6   ...          NaN      NaN   \n",
      "2   738.638748  652.8  345.6   ...          NaN      NaN   \n",
      "3   738.638748  652.8  345.6   ...        652.8    345.6   \n",
      "4   979.011747  883.2 -422.4   ...        883.2   -422.4   \n",
      "5   948.410038  883.2 -345.6   ...          NaN      NaN   \n",
      "6   877.337062  806.4  345.6   ...          NaN      NaN   \n",
      "7   910.331105  806.4 -422.4   ...        806.4   -422.4   \n",
      "8   843.052739  729.6  422.4   ...          NaN      NaN   \n",
      "9   807.313768  729.6  345.6   ...          NaN      NaN   \n",
      "10  877.337062  806.4  345.6   ...          NaN      NaN   \n",
      "11  877.337062  806.4  345.6   ...          NaN      NaN   \n",
      "12  877.337062  806.4  345.6   ...          NaN      NaN   \n",
      "13  877.337062  806.4  345.6   ...          NaN      NaN   \n",
      "14  877.337062  806.4  345.6   ...          NaN      NaN   \n",
      "15  714.281289  576.0  422.4   ...          NaN      NaN   \n",
      "16  463.988965 -192.0 -422.4   ...          NaN      NaN   \n",
      "17  877.337062 -806.4 -345.6   ...          NaN      NaN   \n",
      "\n",
      "                       associated_goes                 starttime_goes  \\\n",
      "0                                 None                           None   \n",
      "1                                 None                           None   \n",
      "2                                 None                           None   \n",
      "3                                 None                           None   \n",
      "4                                 None                           None   \n",
      "5       SOL2010-06-13T05:30:00L121C114  2010-06-13T05:30:00.000000000   \n",
      "6                                 None                           None   \n",
      "7                                 None                           None   \n",
      "8                                 None                           None   \n",
      "9   [[SOL2010-06-13T07:31:00L127C113]]          [1276414260000000000]   \n",
      "10                                None                           None   \n",
      "11                                None                           None   \n",
      "12                                None                           None   \n",
      "13      SOL2010-06-13T13:20:00L103C068  2010-06-13T13:20:00.000000000   \n",
      "14      SOL2010-06-13T18:10:00L102C067  2010-06-13T18:10:00.000000000   \n",
      "15                                None                           None   \n",
      "16                                None                           None   \n",
      "17                                None                           None   \n",
      "\n",
      "                     endtime_goes          peaktime_goes  hpc_x_goes  \\\n",
      "0                            None                   None        None   \n",
      "1                            None                   None        None   \n",
      "2                            None                   None        None   \n",
      "3                            None                   None        None   \n",
      "4                            None                   None        None   \n",
      "5   2010-06-13T05:44:00.000000000    2010-06-13T05:39:00     855.312   \n",
      "6                            None                   None        None   \n",
      "7                            None                   None        None   \n",
      "8                            None                   None        None   \n",
      "9           [1276414680000000000]  [2010-06-13T07:34:00]   [869.712]   \n",
      "10                           None                   None        None   \n",
      "11                           None                   None        None   \n",
      "12                           None                   None        None   \n",
      "13  2010-06-13T13:26:00.000000000    2010-06-13T13:23:00     813.612   \n",
      "14  2010-06-13T18:17:00.000000000    2010-06-13T18:13:00     818.514   \n",
      "15                           None                   None        None   \n",
      "16                           None                   None        None   \n",
      "17                           None                   None        None   \n",
      "\n",
      "     hpc_y_goes  goescls  is_goes  \n",
      "0          None     None        0  \n",
      "1          None     None        0  \n",
      "2          None     None        0  \n",
      "3          None     None        0  \n",
      "4          None     None        0  \n",
      "5      -386.158     M1.0        1  \n",
      "6          None     None        0  \n",
      "7          None     None        0  \n",
      "8          None     None        0  \n",
      "9   [-369.4038]   [C1.2]        1  \n",
      "10         None     None        0  \n",
      "11         None     None        0  \n",
      "12         None     None        0  \n",
      "13       349.79     B2.5        1  \n",
      "14      365.302     B1.8        1  \n",
      "15         None     None        0  \n",
      "16         None     None        0  \n",
      "17         None     None        0  \n",
      "\n",
      "[18 rows x 71 columns]\n"
     ]
    }
   ],
   "source": [
    "print goes_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
