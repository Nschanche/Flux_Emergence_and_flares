{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from shapely import wkt\n",
    "from sunpy.time import parse_time\n",
    "import datetime\n",
    "from shapely.geometry import Polygon, Point\n",
    "from astropy import units as u\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unicode2polygon(bbox_array):\n",
    "    if isinstance(bbox_array, basestring):\n",
    "        bbox_array = wkt.loads(bbox_array)\n",
    "    else:\n",
    "        bbox_array = map(lambda x: wkt.loads(x), bbox_array)\n",
    "#         for i, elem in bbox_array:\n",
    "#             bbox_array[i] = wkt.loads(elem)\n",
    "    return bbox_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#may at some point add location of other flares in as well as peak flux\n",
    "def determine_wavelengths(df, index, index2 = None):\n",
    "    if index2 == None:\n",
    "        if df['obs_channelid'].values[index] == 131:\n",
    "            df['is_131'].values[index] = 1\n",
    "            df['fl_peakflux_131'].values[index] = df['fl_peakflux'].values[index]\n",
    "            df['starttime_131'].values[index] = df['event_starttime'].values[index]\n",
    "            df['peaktime_131'].values[index] = df['event_peaktime'].values[index]\n",
    "            df['endtime_131'].values[index] = df['event_endtime'].values[index]\n",
    "            df['hpc_x_131'].values[index] = df['hpc_x'].values[index]\n",
    "            df['hpc_y_131'].values[index] = df['hpc_y'].values[index]\n",
    "            \n",
    "        if df['obs_channelid'].values[index] == 171:\n",
    "            df['is_171'].values[index] = 1\n",
    "            df['fl_peakflux_171'].values[index] = df['fl_peakflux'].values[index]\n",
    "            df['starttime_171'].values[index] = df['event_starttime'].values[index]\n",
    "            df['peaktime_171'].values[index] = df['event_peaktime'].values[index]\n",
    "            df['endtime_171'].values[index] = df['event_endtime'].values[index]\n",
    "            df['hpc_x_171'].values[index] = df['hpc_x'].values[index]\n",
    "            df['hpc_y_171'].values[index] = df['hpc_y'].values[index]\n",
    "        \n",
    "        if df['obs_channelid'].values[index] == 193:\n",
    "            df['is_193'].values[index] = 1\n",
    "            df['fl_peakflux_193'].values[index] = df['fl_peakflux'].values[index]\n",
    "            df['starttime_193'].values[index] = df['event_starttime'].values[index]\n",
    "            df['peaktime_193'].values[index] = df['event_peaktime'].values[index]\n",
    "            df['endtime_193'].values[index] = df['event_endtime'].values[index]\n",
    "            df['hpc_x_193'].values[index] = df['hpc_x'].values[index]\n",
    "            df['hpc_y_193'].values[index] = df['hpc_y'].values[index]\n",
    "        \n",
    "        if df['obs_channelid'].values[index] == 211:\n",
    "            df['is_211'].values[index] = 1\n",
    "            df['fl_peakflux_211'].values[index] = df['fl_peakflux'].values[index]\n",
    "            df['starttime_211'].values[index] = df['event_starttime'].values[index]\n",
    "            df['peaktime_211'].values[index] = df['event_peaktime'].values[index]\n",
    "            df['endtime_211'].values[index] = df['event_endtime'].values[index]\n",
    "            df['hpc_x_211'].values[index] = df['hpc_x'].values[index]\n",
    "            df['hpc_y_211'].values[index] = df['hpc_y'].values[index]\n",
    "        \n",
    "        if df['obs_channelid'].values[index] == 304:\n",
    "            df['is_304'].values[index] = 1\n",
    "            df['fl_peakflux_304'].values[index] = df['fl_peakflux'].values[index]\n",
    "            df['starttime_304'].values[index] = df['event_starttime'].values[index]\n",
    "            df['peaktime_304'].values[index] = df['event_peaktime'].values[index]\n",
    "            df['endtime_304'].values[index] = df['event_endtime'].values[index]\n",
    "            df['hpc_x_304'].values[index] = df['hpc_x'].values[index]\n",
    "            df['hpc_y_304'].values[index] = df['hpc_y'].values[index]\n",
    "        \n",
    "        if df['obs_channelid'].values[index] == 335:\n",
    "            df['is_335'].values[index] = 1\n",
    "            df['fl_peakflux_335'].values[index] = df['fl_peakflux'].values[index]\n",
    "            df['starttime_335'].values[index] = df['event_starttime'].values[index]\n",
    "            df['peaktime_335'].values[index] = df['event_peaktime'].values[index]\n",
    "            df['endtime_335'].values[index] = df['event_endtime'].values[index]\n",
    "            df['hpc_x_335'].values[index] = df['hpc_x'].values[index]\n",
    "            df['hpc_y_335'].values[index] = df['hpc_y'].values[index]\n",
    "        \n",
    "        if df['obs_channelid'].values[index] == 94:\n",
    "            df['is_94'].values[index] = 1\n",
    "            df['fl_peakflux_94'].values[index] = df['fl_peakflux'].values[index]\n",
    "            df['starttime_94'].values[index] = df['event_starttime'].values[index]\n",
    "            df['peaktime_94'].values[index] = df['event_peaktime'].values[index]\n",
    "            df['endtime_94'].values[index] = df['event_endtime'].values[index]\n",
    "            df['hpc_x_94'].values[index] = df['hpc_x'].values[index]\n",
    "            df['hpc_y_94'].values[index] = df['hpc_y'].values[index]\n",
    "    else:\n",
    "        if df['obs_channelid'].values[index2] == 131:\n",
    "            df['is_131'].values[index] = 1\n",
    "            df['fl_peakflux_131'].values[index] = df['fl_peakflux'].values[index2]\n",
    "            df['starttime_131'].values[index] = df['event_starttime'].values[index2]\n",
    "            df['peaktime_131'].values[index] = df['event_peaktime'].values[index2]\n",
    "            df['endtime_131'].values[index] = df['event_endtime'].values[index2]\n",
    "            df['hpc_x_131'].values[index] = df['hpc_x'].values[index2]\n",
    "            df['hpc_y_131'].values[index] = df['hpc_y'].values[index2]\n",
    "        \n",
    "        if df['obs_channelid'].values[index2] == 171:\n",
    "            df['is_171'].values[index] = 1\n",
    "            df['fl_peakflux_171'].values[index] = df['fl_peakflux'].values[index2]\n",
    "            df['starttime_171'].values[index] = df['event_starttime'].values[index2]\n",
    "            df['peaktime_171'].values[index] = df['event_peaktime'].values[index2]\n",
    "            df['endtime_171'].values[index] = df['event_endtime'].values[index2]\n",
    "            df['hpc_x_171'].values[index] = df['hpc_x'].values[index2]\n",
    "            df['hpc_y_171'].values[index] = df['hpc_y'].values[index2]\n",
    "        \n",
    "        if df['obs_channelid'].values[index2] == 193:\n",
    "            df['is_193'].values[index] = 1\n",
    "            df['fl_peakflux_193'].values[index] = df['fl_peakflux'].values[index2]\n",
    "            df['starttime_193'].values[index] = df['event_starttime'].values[index2]\n",
    "            df['peaktime_193'].values[index] = df['event_peaktime'].values[index2]\n",
    "            df['endtime_193'].values[index] = df['event_endtime'].values[index2]\n",
    "            df['hpc_x_193'].values[index] = df['hpc_x'].values[index2]\n",
    "            df['hpc_y_193'].values[index] = df['hpc_y'].values[index2]\n",
    "        \n",
    "        if df['obs_channelid'].values[index2] == 211:\n",
    "            df['is_211'].values[index] = 1\n",
    "            df['fl_peakflux_211'].values[index] = df['fl_peakflux'].values[index2]\n",
    "            df['starttime_211'].values[index] = df['event_starttime'].values[index2]\n",
    "            df['peaktime_211'].values[index] = df['event_peaktime'].values[index2]\n",
    "            df['endtime_211'].values[index] = df['event_endtime'].values[index2]\n",
    "            df['hpc_x_211'].values[index] = df['hpc_x'].values[index2]\n",
    "            df['hpc_y_211'].values[index] = df['hpc_y'].values[index2]\n",
    "           \n",
    "        if df['obs_channelid'].values[index2] == 304:\n",
    "            df['is_304'].values[index] = 1\n",
    "            df['fl_peakflux_304'].values[index] = df['fl_peakflux'].values[index2]\n",
    "            df['starttime_304'].values[index] = df['event_starttime'].values[index2]\n",
    "            df['peaktime_304'].values[index] = df['event_peaktime'].values[index2]\n",
    "            df['endtime_304'].values[index] = df['event_endtime'].values[index2]\n",
    "            df['hpc_x_304'].values[index] = df['hpc_x'].values[index2]\n",
    "            df['hpc_y_304'].values[index] = df['hpc_y'].values[index2]\n",
    "        \n",
    "        if df['obs_channelid'].values[index2] == 335:\n",
    "            df['is_335'].values[index] = 1\n",
    "            df['fl_peakflux_335'].values[index] = df['fl_peakflux'].values[index2]\n",
    "            df['starttime_335'].values[index] = df['event_starttime'].values[index2]\n",
    "            df['peaktime_335'].values[index] = df['event_peaktime'].values[index2]\n",
    "            df['endtime_335'].values[index] = df['event_endtime'].values[index2]\n",
    "            df['hpc_x_335'].values[index] = df['hpc_x'].values[index2]\n",
    "            df['hpc_y_335'].values[index] = df['hpc_y'].values[index2]\n",
    "        \n",
    "        if df['obs_channelid'].values[index2] == 94:\n",
    "            df['is_94'].values[index] = 1\n",
    "            df['fl_peakflux_94'].values[index] = df['fl_peakflux'].values[index2]\n",
    "            df['starttime_94'].values[index] = df['event_starttime'].values[index2]\n",
    "            df['peaktime_94'].values[index] = df['event_peaktime'].values[index2]\n",
    "            df['endtime_94'].values[index] = df['event_endtime'].values[index2]\n",
    "            df['hpc_x_94'].values[index] = df['hpc_x'].values[index2]\n",
    "            df['hpc_y_94'].values[index] = df['hpc_y'].values[index2]\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fl_duplicates_detective(fl_detective, delta_t_max, distance_max, search_index):\n",
    "    delete_index = []\n",
    "    num_del = 0\n",
    "    num_duplicates = 0\n",
    "    i = 0\n",
    "    associated_fl = []\n",
    "    detective_peak = map(parse_time, fl_detective['event_peaktime'])\n",
    "    detective_position = map(lambda x: wkt.loads(x), fl_detective['hpc_coord'])\n",
    "    delta_t_max = datetime.timedelta(minutes = delta_t_max)\n",
    "    num_events = fl_detective.shape[0]\n",
    "    \n",
    "    #create columns in dataframe for recording multiple wavelength components of flares\n",
    "    zero_list = [0 for n in range(num_events)]\n",
    "    none_list = [None for n in range(num_events)]\n",
    "    is_associated_fl = zero_list\n",
    "    keywords_flare_zeroes = list(np.genfromtxt('keywords_flare_zeroes.csv', delimiter=',', dtype=str))\n",
    "    keywords_flare_nones = list(np.genfromtxt('keywords_flare_nones.csv', delimiter=',', dtype=str))\n",
    "    for elem in keywords_flare_zeroes:\n",
    "        fl_detective.loc[:, elem] = zero_list  \n",
    "    \n",
    "    for elem in keywords_flare_nones:\n",
    "        fl_detective.loc[:, elem] = none_list\n",
    "    \n",
    "    for elem in detective_peak:\n",
    "        fl_match = []\n",
    "        fl_detective = determine_wavelengths(fl_detective, i)\n",
    "        #test whether next entry is redundant\n",
    "        if i < num_events-1:\n",
    "            if fl_detective['obs_channelid'].values[i] == fl_detective['obs_channelid'].values[i+1]:\n",
    "                if detective_peak[i+1]-elem <= delta_t_max:\n",
    "                    if detective_position[i].distance(detective_position[i+1]) <= distance_max:\n",
    "                        delete_index.append(i+1)\n",
    "                        fl_match.append(str(fl_detective['SOL_standard'].values[i+1]))\n",
    "                        num_del+=1\n",
    "        #test whether there are entries in diff wavelengths for the same flare\n",
    "        for j in range(1, search_index):\n",
    "            i2 = i+j\n",
    "            if i2 < (num_events-1):\n",
    "                if fl_detective['obs_channelid'].values[i] != fl_detective['obs_channelid'].values[i2]:\n",
    "                    if detective_peak[i2]-elem <= delta_t_max:\n",
    "                        if detective_position[i].distance(detective_position[i2]) <= distance_max:\n",
    "                            delete_index.append(i2)\n",
    "                            num_duplicates+=1\n",
    "                            fl_match.append(str(fl_detective['SOL_standard'].values[i2]))\n",
    "                            fl_detective = determine_wavelengths(fl_detective, i, i2)\n",
    "        fl_detective.loc[i, 'sum_peakflux'] = (fl_detective.loc[i, 'fl_peakflux_131']+ \n",
    "                                               fl_detective.loc[i, 'fl_peakflux_171']+\n",
    "                                               fl_detective.loc[i, 'fl_peakflux_193']+\n",
    "                                               fl_detective.loc[i, 'fl_peakflux_211']+\n",
    "                                               fl_detective.loc[i, 'fl_peakflux_304']+\n",
    "                                               fl_detective.loc[i, 'fl_peakflux_335']+\n",
    "                                               fl_detective.loc[i, 'fl_peakflux_94'])\n",
    "        if fl_match == []: \n",
    "             fl_match = None\n",
    "        associated_fl.append(fl_match)\n",
    "        i+=1\n",
    "    fl_detective.loc[:, 'associated_fl'] = associated_fl\n",
    "    k = 0\n",
    "    for elem in associated_fl:\n",
    "        if elem!=None:\n",
    "            is_associated_fl[k] = 1\n",
    "        k+=1\n",
    "    delete_index = set(delete_index)\n",
    "    fl_detective.loc[:, 'is_associated_fl'] = is_associated_fl\n",
    "    fl_detective = fl_detective.drop(delete_index) \n",
    "    \n",
    "    print 'original number of events:  %d' % num_events\n",
    "    print 'duplicated events deleted:  %d' % num_del\n",
    "    print 'duplicated events merged:  %d' % (len(delete_index)-num_del)\n",
    "    print 'new number of events:  %d' % (num_events-len(delete_index))\n",
    "    print 'actual new number of events:  %d' %fl_detective.shape[0]\n",
    "\n",
    "    return fl_detective     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original number of events:  87044\n",
      "duplicated events deleted:  5557\n",
      "duplicated events merged:  41194\n",
      "new number of events:  40293\n",
      "actual new number of events:  40293\n"
     ]
    }
   ],
   "source": [
    "flare_keywords = list(np.genfromtxt('keywords_flare.csv', delimiter=',', dtype=str))\n",
    "fl_set = pd.read_csv('raw_detective.csv', delimiter = ',', header = 0, usecols = flare_keywords)\n",
    "fl_set = fl_duplicates_detective(fl_set, 30, 100, 9)\n",
    "flare_keywords_after_merge = list(np.genfromtxt('keywords_flare_after_merge.csv', delimiter=',', dtype=str))\n",
    "fl_set.to_csv('flare_dataset_cleaned_30min_100arcsec.csv', index = False, columns = flare_keywords_after_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_cols(input_file):\n",
    "    df = pd.read_csv(input_file, delimiter = ',', header = 0)\n",
    "    #add a column for distance from flare center to disk center (dist_frm_center)\n",
    "    df = add_dist_frm_center_column(df)\n",
    "    #add column for duration of flare\n",
    "    df.to_csv(input_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "add_cols('flare_dataset_cleaned_30min_100arcsec.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#find the distance from center of disk from x, y coordinates (unit = arcsec)\n",
    "def find_dist_frm_center(x, y):\n",
    "    #set the radius of the Sun\n",
    "    r = 966\n",
    "    #calculate z squared\n",
    "    z_sq = (r**2 - x**2 - y**2)\n",
    "    #if loop to prevent imaginary numbers when taking square root of z squared\n",
    "    if z_sq >= 0: z = z_sq**(0.5)\n",
    "    else: z = (-z_sq)**(0.5)\n",
    "    #calculate the distance to center, disk center @ (0, 0, R)\n",
    "    dist = (x**2 + y**2 + (z-r)**2)**(0.5)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_dist_frm_center_column(df):\n",
    "#     dist_frm_center = []\n",
    "    dist_frm_center = map(lambda x, y: find_dist_frm_center(x, y), df['hpc_x'], df['hpc_y'])\n",
    "#     for i in range(df.shape[0]):\n",
    "#         dist = find_dist_frm_center(df['hpc_x'].values[i], df['hpc_y'].values[i])\n",
    "#         dist_frm_center.append(dist)\n",
    "    df.loc[:, 'dist_frm_center'] = dist_frm_center\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_duration(start_time, stop_time):\n",
    "    duration = ((stop_time - start_time).total_seconds())/60\n",
    "    return duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fl_duplicates_SSW1(fl_SSW):\n",
    "    delete_index = []\n",
    "    i = 0\n",
    "    end = map(parse_time, fl_SSW['event_endtime'])\n",
    "    \n",
    "    for i in range(len(fl_SSW['SOL_standard'])-1):\n",
    "        j = i + 1\n",
    "        if fl_SSW['SOL_standard'].values[i] == fl_SSW['SOL_standard'].values[j]:\n",
    "            if end[i] <= end[j]:\n",
    "                delete_index.append(i)\n",
    "            else: delete_index.append(j)\n",
    "    \n",
    "    fl_SSW = fl_SSW.drop(delete_index)\n",
    "    \n",
    "    print len(delete_index)\n",
    "    \n",
    "    return fl_SSW    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fl_SSW = pd.read_csv('raw_SSW.csv', delimiter = ',', header = 0 )\n",
    "fl_duplicates_SSW = fl_duplicates_SSW1(fl_SSW)\n",
    "keywords_SSW = list(np.genfromtxt('keywords_SSW_flare.csv', delimiter=',', dtype=str))\n",
    "fl_duplicates_SSW.to_csv('prepped_SSW.csv', index = False, columns = keywords_SSW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def associate_GOES(inputFile_fl, inputFile_goes, spatial_sep, output2file=False, out_file = None):\n",
    "    #import a record of flare events as a DataFrame\n",
    "    flare_set = pd.read_csv(inputFile_fl, delimiter = ',', header = 0)\n",
    "    #import a record of GOES events as a DataFrame\n",
    "    goes_set = pd.read_csv(inputFile_goes, delimiter = ',', header = 0)\n",
    "    #set solar radius\n",
    "    r = 966\n",
    "    #how many flare events working with\n",
    "    length = flare_set.shape[0]\n",
    "    #list of zeroes with length of number of flare events\n",
    "    zeroes = [0 for i in range(length)]\n",
    "    #list of nulls with length of number of flare events\n",
    "    nones = [None for i in range(length)]\n",
    "    #create columns (filled with zeroes) for tracking associated events\n",
    "    flare_set.loc[:, 'associated_goes'] = nones\n",
    "\n",
    "    goes_set = goes_set.rename(columns={'SOL_standard':'goes_SOL_standard','event_starttime':'starttime_goes', \n",
    "                                        'event_endtime':'endtime_goes', 'event_peaktime':'peaktime_goes',\n",
    "                                        'hpc_x':'hpc_x_goes', 'hpc_y':'hpc_y_goes', 'ar_noaanum': 'goes_noaanum',\n",
    "                                        'fl_goescls':'goescls'})\n",
    "    goes_keywords = ['starttime_goes', 'endtime_goes', 'peaktime_goes', 'hpc_x_goes', 'hpc_y_goes', 'goescls', \n",
    "                     'goes_noaanum']\n",
    "   \n",
    "    #create columns filled with Nones for each GOES keyword\n",
    "    for elem in goes_keywords:\n",
    "         flare_set.loc[:, elem] = nones\n",
    "    #convert start and end times to datetime objects\n",
    "    flare_set['event_starttime'] = map(parse_time, flare_set['event_starttime'])\n",
    "    flare_set['event_endtime'] = map(parse_time, flare_set['event_endtime'])\n",
    "    goes_set['starttime_goes'] = map(parse_time, goes_set['starttime_goes'])\n",
    "    goes_set['endtime_goes'] = map(parse_time, goes_set['endtime_goes'])\n",
    "#     ar_list['start_time'] = map(lambda x, y, z: datetime.datetime(x, y, z), ar_list['start_year'], \n",
    "#                                 ar_list['start_month'], ar_list['start_day'])\n",
    "#     ar_list['end_time'] = map(lambda x, y, z: datetime.datetime(x, y, z), ar_list['end_year'], \n",
    "#                                   ar_list['end_month'], ar_list['end_day'])\n",
    "    #set positional row index \n",
    "    i = -1\n",
    "\n",
    "    for annoying_wrong_obj_type in flare_set['event_starttime']:\n",
    "        i += 1\n",
    "        #print which flare event function is currently processing, so the user has an idea of how much longer\n",
    "        #program will need to run\n",
    "        start = flare_set['event_starttime'].values[i]\n",
    "        end = flare_set['event_endtime'].values[i]\n",
    "        if (i+1)%100 == 0:\n",
    "            print '%d / %d events' %((i+1), length)\n",
    "        #begin eliminating ar events based on temporal parameters\n",
    "        goes_search = goes_set.ix[goes_set['starttime_goes']<=start]\n",
    "        goes_search = goes_search.ix[goes_search['endtime_goes']>=end]\n",
    "        #as long as the temporal search does not eliminate all possible related AR events, proceed\n",
    "        if goes_search.empty==False:\n",
    "                fl_point = Point((flare_set['hpc_x'].values[i], flare_set['hpc_y'].values[i]))\n",
    "                min_s = spatial_sep\n",
    "                event_index = None\n",
    "                found_goes = False\n",
    "                for j in range(goes_search.shape[0]):\n",
    "                    #create a point object from GOES flare's location\n",
    "                    goes_point = Point((goes_search['hpc_x_goes'].values[j], goes_search['hpc_y_goes'].values[j]))\n",
    "                    #calculate the minimum 2D distance between the AIA flare's and GOES flare's mean coordinates\n",
    "                    chord = fl_point.distance(goes_point)\n",
    "                    #calculate the minimum 3D distance along the sun's curved surface between the flare events\n",
    "                    #assumes the same radius for all events\n",
    "                    s = r*np.arcsin(chord/(2*r))\n",
    "                    #determine whether the spatial distance between AIA and GOES flares meets the set parameter\n",
    "                    if s <= spatial_sep:\n",
    "                        #have found an associated GOES flare\n",
    "                        found_goes = True\n",
    "                        if s <= min_s: \n",
    "                            min_s = s\n",
    "                            event_index = j\n",
    "                if found_goes:\n",
    "                    flare_set['associated_goes'].values[i] = goes_search['goes_SOL_standard'].values[event_index]\n",
    "                    for elem in goes_keywords:\n",
    "                        flare_set.loc[i, elem] = goes_search[elem].values[event_index]\n",
    "            \n",
    "    #create boolean var to easily determine whether flare associated with an AR\n",
    "    k = 0\n",
    "    is_goes = [0 for i in range(flare_set.shape[0])]\n",
    "    for elem in flare_set['associated_goes']:\n",
    "        if elem!=None:\n",
    "            is_goes[k] = 1\n",
    "        k+=1\n",
    "    flare_set.loc[:, 'is_goes'] = is_goes\n",
    "    cgs_flux = GOES2numericflux(flare_set['goescls'])\n",
    "    flare_set.loc[:, 'fl_peakflux_goes'] = cgs_flux \n",
    "    \n",
    "    #write dataframe to a csv file depending on initial parameters\n",
    "    if output2file:\n",
    "        if out_file == None:\n",
    "            #create a generic name for file based on search parameters if no file name specified\n",
    "             out_file = inputFile_fl[0:-4]+'_with_GOES1.csv'\n",
    "        #import which keywords to keep for outported data\n",
    "        flare_keywords = list(np.genfromtxt('keywords_flare_after_merge.csv', delimiter=',', dtype=str))\n",
    "        #add to these keywords descriptors of associated GOES flare\n",
    "        flare_keywords.extend(['is_goes','associated_goes', 'fl_peakflux_goes'])\n",
    "        flare_keywords.extend(goes_keywords)\n",
    "        #write to csv\n",
    "        flare_set.to_csv(path_or_buf=out_file, columns = flare_keywords, index = False)\n",
    "        \n",
    "    return flare_set\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 / 40293 events\n",
      "200 / 40293 events\n",
      "300 / 40293 events\n",
      "400 / 40293 events\n",
      "500 / 40293 events\n",
      "600 / 40293 events\n",
      "700 / 40293 events\n",
      "800 / 40293 events\n",
      "900 / 40293 events\n",
      "1000 / 40293 events\n",
      "1100 / 40293 events\n",
      "1200 / 40293 events\n",
      "1300 / 40293 events\n",
      "1400 / 40293 events\n",
      "1500 / 40293 events\n",
      "1600 / 40293 events\n",
      "1700 / 40293 events\n",
      "1800 / 40293 events\n",
      "1900 / 40293 events\n",
      "2000 / 40293 events\n",
      "2100 / 40293 events\n",
      "2200 / 40293 events\n",
      "2300 / 40293 events\n",
      "2400 / 40293 events\n",
      "2500 / 40293 events\n",
      "2600 / 40293 events\n",
      "2700 / 40293 events\n",
      "2800 / 40293 events\n",
      "2900 / 40293 events\n",
      "3000 / 40293 events\n",
      "3100 / 40293 events\n",
      "3200 / 40293 events\n",
      "3300 / 40293 events\n",
      "3400 / 40293 events\n",
      "3500 / 40293 events\n",
      "3600 / 40293 events\n",
      "3700 / 40293 events\n",
      "3800 / 40293 events\n",
      "3900 / 40293 events\n",
      "4000 / 40293 events\n",
      "4100 / 40293 events\n",
      "4200 / 40293 events\n",
      "4300 / 40293 events\n",
      "4400 / 40293 events\n",
      "4500 / 40293 events\n",
      "4600 / 40293 events\n",
      "4700 / 40293 events\n",
      "4800 / 40293 events\n",
      "4900 / 40293 events\n",
      "5000 / 40293 events\n",
      "5100 / 40293 events\n",
      "5200 / 40293 events\n",
      "5300 / 40293 events\n",
      "5400 / 40293 events\n",
      "5500 / 40293 events\n",
      "5600 / 40293 events\n",
      "5700 / 40293 events\n",
      "5800 / 40293 events\n",
      "5900 / 40293 events\n",
      "6000 / 40293 events\n",
      "6100 / 40293 events\n",
      "6200 / 40293 events\n",
      "6300 / 40293 events\n",
      "6400 / 40293 events\n",
      "6500 / 40293 events\n",
      "6600 / 40293 events\n",
      "6700 / 40293 events\n",
      "6800 / 40293 events\n",
      "6900 / 40293 events\n",
      "7000 / 40293 events\n",
      "7100 / 40293 events\n",
      "7200 / 40293 events\n",
      "7300 / 40293 events\n",
      "7400 / 40293 events\n",
      "7500 / 40293 events\n",
      "7600 / 40293 events\n",
      "7700 / 40293 events\n",
      "7800 / 40293 events\n",
      "7900 / 40293 events\n",
      "8000 / 40293 events\n",
      "8100 / 40293 events\n",
      "8200 / 40293 events\n",
      "8300 / 40293 events\n",
      "8400 / 40293 events\n",
      "8500 / 40293 events\n",
      "8600 / 40293 events\n",
      "8700 / 40293 events\n",
      "8800 / 40293 events\n",
      "8900 / 40293 events\n",
      "9000 / 40293 events\n",
      "9100 / 40293 events\n",
      "9200 / 40293 events\n",
      "9300 / 40293 events\n",
      "9400 / 40293 events\n",
      "9500 / 40293 events\n",
      "9600 / 40293 events\n",
      "9700 / 40293 events\n",
      "9800 / 40293 events\n",
      "9900 / 40293 events\n",
      "10000 / 40293 events\n",
      "10100 / 40293 events\n",
      "10200 / 40293 events\n",
      "10300 / 40293 events\n",
      "10400 / 40293 events\n",
      "10500 / 40293 events\n",
      "10600 / 40293 events\n",
      "10700 / 40293 events\n",
      "10800 / 40293 events\n",
      "10900 / 40293 events\n",
      "11000 / 40293 events\n",
      "11100 / 40293 events\n",
      "11200 / 40293 events\n",
      "11300 / 40293 events\n",
      "11400 / 40293 events\n",
      "11500 / 40293 events\n",
      "11600 / 40293 events\n",
      "11700 / 40293 events\n",
      "11800 / 40293 events\n",
      "11900 / 40293 events\n",
      "12000 / 40293 events\n",
      "12100 / 40293 events\n",
      "12200 / 40293 events\n",
      "12300 / 40293 events\n",
      "12400 / 40293 events\n",
      "12500 / 40293 events\n",
      "12600 / 40293 events\n",
      "12700 / 40293 events\n",
      "12800 / 40293 events\n",
      "12900 / 40293 events\n",
      "13000 / 40293 events\n",
      "13100 / 40293 events\n",
      "13200 / 40293 events\n",
      "13300 / 40293 events\n",
      "13400 / 40293 events\n",
      "13500 / 40293 events\n",
      "13600 / 40293 events\n",
      "13700 / 40293 events\n",
      "13800 / 40293 events\n",
      "13900 / 40293 events\n",
      "14000 / 40293 events\n",
      "14100 / 40293 events\n",
      "14200 / 40293 events\n",
      "14300 / 40293 events\n",
      "14400 / 40293 events\n",
      "14500 / 40293 events\n",
      "14600 / 40293 events\n",
      "14700 / 40293 events\n",
      "14800 / 40293 events\n",
      "14900 / 40293 events\n",
      "15000 / 40293 events\n",
      "15100 / 40293 events\n",
      "15200 / 40293 events\n",
      "15300 / 40293 events\n",
      "15400 / 40293 events\n",
      "15500 / 40293 events\n",
      "15600 / 40293 events\n",
      "15700 / 40293 events\n",
      "15800 / 40293 events\n",
      "15900 / 40293 events\n",
      "16000 / 40293 events\n",
      "16100 / 40293 events\n",
      "16200 / 40293 events\n",
      "16300 / 40293 events\n",
      "16400 / 40293 events\n",
      "16500 / 40293 events\n",
      "16600 / 40293 events\n",
      "16700 / 40293 events\n",
      "16800 / 40293 events\n",
      "16900 / 40293 events\n",
      "17000 / 40293 events\n",
      "17100 / 40293 events\n",
      "17200 / 40293 events\n",
      "17300 / 40293 events\n",
      "17400 / 40293 events\n",
      "17500 / 40293 events\n",
      "17600 / 40293 events\n",
      "17700 / 40293 events\n",
      "17800 / 40293 events\n",
      "17900 / 40293 events\n",
      "18000 / 40293 events\n",
      "18100 / 40293 events\n",
      "18200 / 40293 events\n",
      "18300 / 40293 events\n",
      "18400 / 40293 events\n",
      "18500 / 40293 events\n",
      "18600 / 40293 events\n",
      "18700 / 40293 events\n",
      "18800 / 40293 events\n",
      "18900 / 40293 events\n",
      "19000 / 40293 events\n",
      "19100 / 40293 events\n",
      "19200 / 40293 events\n",
      "19300 / 40293 events\n",
      "19400 / 40293 events\n",
      "19500 / 40293 events\n",
      "19600 / 40293 events\n",
      "19700 / 40293 events\n",
      "19800 / 40293 events\n",
      "19900 / 40293 events\n",
      "20000 / 40293 events\n",
      "20100 / 40293 events\n",
      "20200 / 40293 events\n",
      "20300 / 40293 events\n",
      "20400 / 40293 events\n",
      "20500 / 40293 events\n",
      "20600 / 40293 events\n",
      "20700 / 40293 events\n",
      "20800 / 40293 events\n",
      "20900 / 40293 events\n",
      "21000 / 40293 events\n",
      "21100 / 40293 events\n",
      "21200 / 40293 events\n",
      "21300 / 40293 events\n",
      "21400 / 40293 events\n",
      "21500 / 40293 events\n",
      "21600 / 40293 events\n",
      "21700 / 40293 events\n",
      "21800 / 40293 events\n",
      "21900 / 40293 events\n",
      "22000 / 40293 events\n",
      "22100 / 40293 events\n",
      "22200 / 40293 events\n",
      "22300 / 40293 events\n",
      "22400 / 40293 events\n",
      "22500 / 40293 events\n",
      "22600 / 40293 events\n",
      "22700 / 40293 events\n",
      "22800 / 40293 events\n",
      "22900 / 40293 events\n",
      "23000 / 40293 events\n",
      "23100 / 40293 events\n",
      "23200 / 40293 events\n",
      "23300 / 40293 events\n",
      "23400 / 40293 events\n",
      "23500 / 40293 events\n",
      "23600 / 40293 events\n",
      "23700 / 40293 events\n",
      "23800 / 40293 events\n",
      "23900 / 40293 events\n",
      "24000 / 40293 events\n",
      "24100 / 40293 events\n",
      "24200 / 40293 events\n",
      "24300 / 40293 events\n",
      "24400 / 40293 events\n",
      "24500 / 40293 events\n",
      "24600 / 40293 events\n",
      "24700 / 40293 events\n",
      "24800 / 40293 events\n",
      "24900 / 40293 events\n",
      "25000 / 40293 events\n",
      "25100 / 40293 events\n",
      "25200 / 40293 events\n",
      "25300 / 40293 events\n",
      "25400 / 40293 events\n",
      "25500 / 40293 events\n",
      "25600 / 40293 events\n",
      "25700 / 40293 events\n",
      "25800 / 40293 events\n",
      "25900 / 40293 events\n",
      "26000 / 40293 events\n",
      "26100 / 40293 events\n",
      "26200 / 40293 events\n",
      "26300 / 40293 events\n",
      "26400 / 40293 events\n",
      "26500 / 40293 events\n",
      "26600 / 40293 events\n",
      "26700 / 40293 events\n",
      "26800 / 40293 events\n",
      "26900 / 40293 events\n",
      "27000 / 40293 events\n",
      "27100 / 40293 events\n",
      "27200 / 40293 events\n",
      "27300 / 40293 events\n",
      "27400 / 40293 events\n",
      "27500 / 40293 events\n",
      "27600 / 40293 events\n",
      "27700 / 40293 events\n",
      "27800 / 40293 events\n",
      "27900 / 40293 events\n",
      "28000 / 40293 events\n",
      "28100 / 40293 events\n",
      "28200 / 40293 events\n",
      "28300 / 40293 events\n",
      "28400 / 40293 events\n",
      "28500 / 40293 events\n",
      "28600 / 40293 events\n",
      "28700 / 40293 events\n",
      "28800 / 40293 events\n",
      "28900 / 40293 events\n",
      "29000 / 40293 events\n",
      "29100 / 40293 events\n",
      "29200 / 40293 events\n",
      "29300 / 40293 events\n",
      "29400 / 40293 events\n",
      "29500 / 40293 events\n",
      "29600 / 40293 events\n",
      "29700 / 40293 events\n",
      "29800 / 40293 events\n",
      "29900 / 40293 events\n",
      "30000 / 40293 events\n",
      "30100 / 40293 events\n",
      "30200 / 40293 events\n",
      "30300 / 40293 events\n",
      "30400 / 40293 events\n",
      "30500 / 40293 events\n",
      "30600 / 40293 events\n",
      "30700 / 40293 events\n",
      "30800 / 40293 events\n",
      "30900 / 40293 events\n",
      "31000 / 40293 events\n",
      "31100 / 40293 events\n",
      "31200 / 40293 events\n",
      "31300 / 40293 events\n",
      "31400 / 40293 events\n",
      "31500 / 40293 events\n",
      "31600 / 40293 events\n",
      "31700 / 40293 events\n",
      "31800 / 40293 events\n",
      "31900 / 40293 events\n",
      "32000 / 40293 events\n",
      "32100 / 40293 events\n",
      "32200 / 40293 events\n",
      "32300 / 40293 events\n",
      "32400 / 40293 events\n",
      "32500 / 40293 events\n",
      "32600 / 40293 events\n",
      "32700 / 40293 events\n",
      "32800 / 40293 events\n",
      "32900 / 40293 events\n",
      "33000 / 40293 events\n",
      "33100 / 40293 events\n",
      "33200 / 40293 events\n",
      "33300 / 40293 events\n",
      "33400 / 40293 events\n",
      "33500 / 40293 events\n",
      "33600 / 40293 events\n",
      "33700 / 40293 events\n",
      "33800 / 40293 events\n",
      "33900 / 40293 events\n",
      "34000 / 40293 events\n",
      "34100 / 40293 events\n",
      "34200 / 40293 events\n",
      "34300 / 40293 events\n",
      "34400 / 40293 events\n",
      "34500 / 40293 events\n",
      "34600 / 40293 events\n",
      "34700 / 40293 events\n",
      "34800 / 40293 events\n",
      "34900 / 40293 events\n",
      "35000 / 40293 events\n",
      "35100 / 40293 events\n",
      "35200 / 40293 events\n",
      "35300 / 40293 events\n",
      "35400 / 40293 events\n",
      "35500 / 40293 events\n",
      "35600 / 40293 events\n",
      "35700 / 40293 events\n",
      "35800 / 40293 events\n",
      "35900 / 40293 events\n",
      "36000 / 40293 events\n",
      "36100 / 40293 events\n",
      "36200 / 40293 events\n",
      "36300 / 40293 events\n",
      "36400 / 40293 events\n",
      "36500 / 40293 events\n",
      "36600 / 40293 events\n",
      "36700 / 40293 events\n",
      "36800 / 40293 events\n",
      "36900 / 40293 events\n",
      "37000 / 40293 events\n",
      "37100 / 40293 events\n",
      "37200 / 40293 events\n",
      "37300 / 40293 events\n",
      "37400 / 40293 events\n",
      "37500 / 40293 events\n",
      "37600 / 40293 events\n",
      "37700 / 40293 events\n",
      "37800 / 40293 events\n",
      "37900 / 40293 events\n",
      "38000 / 40293 events\n",
      "38100 / 40293 events\n",
      "38200 / 40293 events\n",
      "38300 / 40293 events\n",
      "38400 / 40293 events\n",
      "38500 / 40293 events\n",
      "38600 / 40293 events\n",
      "38700 / 40293 events\n",
      "38800 / 40293 events\n",
      "38900 / 40293 events\n",
      "39000 / 40293 events\n",
      "39100 / 40293 events\n",
      "39200 / 40293 events\n",
      "39300 / 40293 events\n",
      "39400 / 40293 events\n",
      "39500 / 40293 events\n",
      "39600 / 40293 events\n",
      "39700 / 40293 events\n",
      "39800 / 40293 events\n",
      "39900 / 40293 events\n",
      "40000 / 40293 events\n",
      "40100 / 40293 events\n",
      "40200 / 40293 events\n"
     ]
    }
   ],
   "source": [
    "#associate_GOES(inputFile_fl, inputFile_goes, spatial_sep, output2file=False, out_file = None)\n",
    "goes_output = associate_GOES('flare_dataset_cleaned_30min_100arcsec.csv', 'prepped_GOES.csv', 200, output2file=True, \n",
    "                             out_file = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert GOES classification into a numerical flux value\n",
    "#GOES in w/m^2\n",
    "\n",
    "#A < 10^-7\n",
    "#B 10^-7 — 10^-6\n",
    "#C 10^-6 — 10^-5\n",
    "#M 10^-5 — 10^-4\n",
    "#X > 10^-4\n",
    "\n",
    "#W to erg/s -> 10^7\n",
    "#m^2 to cm^2 -> 10^4\n",
    "\n",
    "def GOES2numericflux(column):\n",
    "    column = map(str, column)\n",
    "    alphaclass = map(lambda x: x[0], column)\n",
    "    numclass = map(lambda x: x[1:], column)\n",
    "    flux = map(lambda x, y: alpha2baseflux(x)*ignore_nans(y), alphaclass, numclass)\n",
    "    flux_cgs = map(lambda x: int(x*10**7*10**4), flux)\n",
    "    return flux_cgs\n",
    "    \n",
    "def alpha2baseflux(alpha):\n",
    "#     if alpha == 'n':\n",
    "#         baseflux = 0\n",
    "    if alpha == 'A':\n",
    "        baseflux = 10**(-8)\n",
    "    elif alpha == 'B':\n",
    "        baseflux = 10**(-7)\n",
    "    elif alpha == 'C':\n",
    "        baseflux = 10**(-6)\n",
    "    elif alpha == 'M':\n",
    "        baseflux = 10**(-5)\n",
    "    elif alpha == 'X':\n",
    "        baseflux = 10**(-4)\n",
    "    else: baseflux = 0\n",
    "    return baseflux\n",
    "\n",
    "def ignore_nans(number):\n",
    "    if number=='an' or number=='one':\n",
    "        number = 0\n",
    "    else: number = float(number)\n",
    "    return number    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2723: DtypeWarning: Columns (109) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "flare_set = pd.read_csv('flare_dataset_cleaned_30min_100arcsec_with_GOES_with_ar_with_sigmoid.csv', delimiter = ',', \n",
    "                        header = 0)\n",
    "goescls = flare_set['goescls']\n",
    "cgs_flux = GOES2numericflux(goescls)\n",
    "flare_set.loc[:, 'fl_peakflux_goes'] = cgs_flux \n",
    "flare_set.to_csv('flare_dataset_cleaned_30min_100arcsec_with_GOES_with_ar_with_sigmoid.csv', index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
