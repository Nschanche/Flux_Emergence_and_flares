{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from shapely import wkt\n",
    "from sunpy.time import parse_time\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unicode2polygon(bbox_array):\n",
    "    if isinstance(bbox_array, basestring):\n",
    "        bbox_array = wkt.loads(bbox_array)\n",
    "    else:\n",
    "        bbox_array = map(lambda x: wkt.loads(x), bbox_array)\n",
    "#         for i, elem in bbox_array:\n",
    "#             bbox_array[i] = wkt.loads(elem)\n",
    "    return bbox_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#may at some point add location of other flares in as well as peak flux\n",
    "def determine_wavelengths(df, index, index2 = None):\n",
    "    if index2 == None:\n",
    "        if df['obs_channelid'].values[index] == 131:\n",
    "            df['is_131'].values[index] = 1\n",
    "            df['fl_peakflux_131'].values[index] = df['fl_peakflux'].values[index]\n",
    "            df['starttime_131'].values[index] = df['event_starttime'].values[index]\n",
    "            df['peaktime_131'].values[index] = df['event_peaktime'].values[index]\n",
    "            df['endtime_131'].values[index] = df['event_endtime'].values[index]\n",
    "            df['hpc_x_131'].values[index] = df['hpc_x'].values[index]\n",
    "            df['hpc_y_131'].values[index] = df['hpc_y'].values[index]\n",
    "            \n",
    "        if df['obs_channelid'].values[index] == 171:\n",
    "            df['is_171'].values[index] = 1\n",
    "            df['fl_peakflux_171'].values[index] = df['fl_peakflux'].values[index]\n",
    "            df['starttime_171'].values[index] = df['event_starttime'].values[index]\n",
    "            df['peaktime_171'].values[index] = df['event_peaktime'].values[index]\n",
    "            df['endtime_171'].values[index] = df['event_endtime'].values[index]\n",
    "            df['hpc_x_171'].values[index] = df['hpc_x'].values[index]\n",
    "            df['hpc_y_171'].values[index] = df['hpc_y'].values[index]\n",
    "        \n",
    "        if df['obs_channelid'].values[index] == 193:\n",
    "            df['is_193'].values[index] = 1\n",
    "            df['fl_peakflux_193'].values[index] = df['fl_peakflux'].values[index]\n",
    "            df['starttime_193'].values[index] = df['event_starttime'].values[index]\n",
    "            df['peaktime_193'].values[index] = df['event_peaktime'].values[index]\n",
    "            df['endtime_193'].values[index] = df['event_endtime'].values[index]\n",
    "            df['hpc_x_193'].values[index] = df['hpc_x'].values[index]\n",
    "            df['hpc_y_193'].values[index] = df['hpc_y'].values[index]\n",
    "        \n",
    "        if df['obs_channelid'].values[index] == 211:\n",
    "            df['is_211'].values[index] = 1\n",
    "            df['fl_peakflux_211'].values[index] = df['fl_peakflux'].values[index]\n",
    "            df['starttime_211'].values[index] = df['event_starttime'].values[index]\n",
    "            df['peaktime_211'].values[index] = df['event_peaktime'].values[index]\n",
    "            df['endtime_211'].values[index] = df['event_endtime'].values[index]\n",
    "            df['hpc_x_211'].values[index] = df['hpc_x'].values[index]\n",
    "            df['hpc_y_211'].values[index] = df['hpc_y'].values[index]\n",
    "        \n",
    "        if df['obs_channelid'].values[index] == 304:\n",
    "            df['is_304'].values[index] = 1\n",
    "            df['fl_peakflux_304'].values[index] = df['fl_peakflux'].values[index]\n",
    "            df['starttime_304'].values[index] = df['event_starttime'].values[index]\n",
    "            df['peaktime_304'].values[index] = df['event_peaktime'].values[index]\n",
    "            df['endtime_304'].values[index] = df['event_endtime'].values[index]\n",
    "            df['hpc_x_304'].values[index] = df['hpc_x'].values[index]\n",
    "            df['hpc_y_304'].values[index] = df['hpc_y'].values[index]\n",
    "        \n",
    "        if df['obs_channelid'].values[index] == 335:\n",
    "            df['is_335'].values[index] = 1\n",
    "            df['fl_peakflux_335'].values[index] = df['fl_peakflux'].values[index]\n",
    "            df['starttime_335'].values[index] = df['event_starttime'].values[index]\n",
    "            df['peaktime_335'].values[index] = df['event_peaktime'].values[index]\n",
    "            df['endtime_335'].values[index] = df['event_endtime'].values[index]\n",
    "            df['hpc_x_335'].values[index] = df['hpc_x'].values[index]\n",
    "            df['hpc_y_335'].values[index] = df['hpc_y'].values[index]\n",
    "        \n",
    "        if df['obs_channelid'].values[index] == 94:\n",
    "            df['is_94'].values[index] = 1\n",
    "            df['fl_peakflux_94'].values[index] = df['fl_peakflux'].values[index]\n",
    "            df['starttime_94'].values[index] = df['event_starttime'].values[index]\n",
    "            df['peaktime_94'].values[index] = df['event_peaktime'].values[index]\n",
    "            df['endtime_94'].values[index] = df['event_endtime'].values[index]\n",
    "            df['hpc_x_94'].values[index] = df['hpc_x'].values[index]\n",
    "            df['hpc_y_94'].values[index] = df['hpc_y'].values[index]\n",
    "    else:\n",
    "        if df['obs_channelid'].values[index2] == 131:\n",
    "            df['is_131'].values[index] = 1\n",
    "            df['fl_peakflux_131'].values[index] = df['fl_peakflux'].values[index2]\n",
    "            df['starttime_131'].values[index] = df['event_starttime'].values[index2]\n",
    "            df['peaktime_131'].values[index] = df['event_peaktime'].values[index2]\n",
    "            df['endtime_131'].values[index] = df['event_endtime'].values[index2]\n",
    "            df['hpc_x_131'].values[index] = df['hpc_x'].values[index2]\n",
    "            df['hpc_y_131'].values[index] = df['hpc_y'].values[index2]\n",
    "        \n",
    "        if df['obs_channelid'].values[index2] == 171:\n",
    "            df['is_171'].values[index] = 1\n",
    "            df['fl_peakflux_171'].values[index] = df['fl_peakflux'].values[index2]\n",
    "            df['starttime_171'].values[index] = df['event_starttime'].values[index2]\n",
    "            df['peaktime_171'].values[index] = df['event_peaktime'].values[index2]\n",
    "            df['endtime_171'].values[index] = df['event_endtime'].values[index2]\n",
    "            df['hpc_x_171'].values[index] = df['hpc_x'].values[index2]\n",
    "            df['hpc_y_171'].values[index] = df['hpc_y'].values[index2]\n",
    "        \n",
    "        if df['obs_channelid'].values[index2] == 193:\n",
    "            df['is_193'].values[index] = 1\n",
    "            df['fl_peakflux_193'].values[index] = df['fl_peakflux'].values[index2]\n",
    "            df['starttime_193'].values[index] = df['event_starttime'].values[index2]\n",
    "            df['peaktime_193'].values[index] = df['event_peaktime'].values[index2]\n",
    "            df['endtime_193'].values[index] = df['event_endtime'].values[index2]\n",
    "            df['hpc_x_193'].values[index] = df['hpc_x'].values[index2]\n",
    "            df['hpc_y_193'].values[index] = df['hpc_y'].values[index2]\n",
    "        \n",
    "        if df['obs_channelid'].values[index2] == 211:\n",
    "            df['is_211'].values[index] = 1\n",
    "            df['fl_peakflux_211'].values[index] = df['fl_peakflux'].values[index2]\n",
    "            df['starttime_211'].values[index] = df['event_starttime'].values[index2]\n",
    "            df['peaktime_211'].values[index] = df['event_peaktime'].values[index2]\n",
    "            df['endtime_211'].values[index] = df['event_endtime'].values[index2]\n",
    "            df['hpc_x_211'].values[index] = df['hpc_x'].values[index2]\n",
    "            df['hpc_y_211'].values[index] = df['hpc_y'].values[index2]\n",
    "           \n",
    "        if df['obs_channelid'].values[index2] == 304:\n",
    "            df['is_304'].values[index] = 1\n",
    "            df['fl_peakflux_304'].values[index] = df['fl_peakflux'].values[index2]\n",
    "            df['starttime_304'].values[index] = df['event_starttime'].values[index2]\n",
    "            df['peaktime_304'].values[index] = df['event_peaktime'].values[index2]\n",
    "            df['endtime_304'].values[index] = df['event_endtime'].values[index2]\n",
    "            df['hpc_x_304'].values[index] = df['hpc_x'].values[index2]\n",
    "            df['hpc_y_304'].values[index] = df['hpc_y'].values[index2]\n",
    "        \n",
    "        if df['obs_channelid'].values[index2] == 335:\n",
    "            df['is_335'].values[index] = 1\n",
    "            df['fl_peakflux_335'].values[index] = df['fl_peakflux'].values[index2]\n",
    "            df['starttime_335'].values[index] = df['event_starttime'].values[index2]\n",
    "            df['peaktime_335'].values[index] = df['event_peaktime'].values[index2]\n",
    "            df['endtime_335'].values[index] = df['event_endtime'].values[index2]\n",
    "            df['hpc_x_335'].values[index] = df['hpc_x'].values[index2]\n",
    "            df['hpc_y_335'].values[index] = df['hpc_y'].values[index2]\n",
    "        \n",
    "        if df['obs_channelid'].values[index2] == 94:\n",
    "            df['is_94'].values[index] = 1\n",
    "            df['fl_peakflux_94'].values[index] = df['fl_peakflux'].values[index2]\n",
    "            df['starttime_94'].values[index] = df['event_starttime'].values[index2]\n",
    "            df['peaktime_94'].values[index] = df['event_peaktime'].values[index2]\n",
    "            df['endtime_94'].values[index] = df['event_endtime'].values[index2]\n",
    "            df['hpc_x_94'].values[index] = df['hpc_x'].values[index2]\n",
    "            df['hpc_y_94'].values[index] = df['hpc_y'].values[index2]\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fl_duplicates_detective(fl_detective, delta_t_max, distance_max, search_index):\n",
    "    delete_index = []\n",
    "    num_del = 0\n",
    "    num_duplicates = 0\n",
    "    i = 0\n",
    "    associated_fl = []\n",
    "    detective_peak = map(parse_time, fl_detective['event_peaktime'])\n",
    "    detective_position = map(lambda x: wkt.loads(x), fl_detective['hpc_coord'])\n",
    "    delta_t_max = datetime.timedelta(minutes = delta_t_max)\n",
    "    num_events = fl_detective.shape[0]\n",
    "    \n",
    "    #create columns in dataframe for recording multiple wavelength components of flares\n",
    "    zero_list = [0 for n in range(num_events)]\n",
    "    none_list = [None for n in range(num_events)]\n",
    "    is_associated_fl = zero_list\n",
    "    keywords_flare_zeroes = list(np.genfromtxt('keywords_flare_zeroes.csv', delimiter=',', dtype=str))\n",
    "    keywords_flare_nones = list(np.genfromtxt('keywords_flare_nones.csv', delimiter=',', dtype=str))\n",
    "    for elem in keywords_flare_zeroes:\n",
    "        fl_detective.loc[:, elem] = zero_list  \n",
    "    \n",
    "    for elem in keywords_flare_nones:\n",
    "        fl_detective.loc[:, elem] = none_list\n",
    "    \n",
    "    for elem in detective_peak:\n",
    "        fl_match = []\n",
    "        fl_detective = determine_wavelengths(fl_detective, i)\n",
    "        #test whether next entry is redundant\n",
    "        if i < num_events-1:\n",
    "            if fl_detective['obs_channelid'].values[i] == fl_detective['obs_channelid'].values[i+1]:\n",
    "                if detective_peak[i+1]-elem <= delta_t_max:\n",
    "                    if detective_position[i].distance(detective_position[i+1]) <= distance_max:\n",
    "                        delete_index.append(i+1)\n",
    "                        fl_match.append(str(fl_detective['SOL_standard'].values[i+1]))\n",
    "                        num_del+=1\n",
    "        #test whether there are entries in diff wavelengths for the same flare\n",
    "        for j in range(1, search_index):\n",
    "            i2 = i+j\n",
    "            if i2 < (num_events-1):\n",
    "                if fl_detective['obs_channelid'].values[i] != fl_detective['obs_channelid'].values[i2]:\n",
    "                    if detective_peak[i2]-elem <= delta_t_max:\n",
    "                        if detective_position[i].distance(detective_position[i2]) <= distance_max:\n",
    "                            delete_index.append(i2)\n",
    "                            num_duplicates+=1\n",
    "                            fl_match.append(str(fl_detective['SOL_standard'].values[i2]))\n",
    "                            fl_detective = determine_wavelengths(fl_detective, i, i2)\n",
    "        fl_detective.loc[i, 'sum_peakflux'] = (fl_detective.loc[i, 'fl_peakflux_131']+ \n",
    "                                               fl_detective.loc[i, 'fl_peakflux_171']+\n",
    "                                               fl_detective.loc[i, 'fl_peakflux_193']+\n",
    "                                               fl_detective.loc[i, 'fl_peakflux_211']+\n",
    "                                               fl_detective.loc[i, 'fl_peakflux_304']+\n",
    "                                               fl_detective.loc[i, 'fl_peakflux_335']+\n",
    "                                               fl_detective.loc[i, 'fl_peakflux_94'])\n",
    "        if fl_match == []: \n",
    "             fl_match = None\n",
    "        associated_fl.append(fl_match)\n",
    "        i+=1\n",
    "    fl_detective.loc[:, 'associated_fl'] = associated_fl\n",
    "    k = 0\n",
    "    for elem in associated_fl:\n",
    "        if elem!=None:\n",
    "            is_associated_fl[k] = 1\n",
    "        k+=1\n",
    "    delete_index = set(delete_index)\n",
    "    fl_detective.loc[:, 'is_associated_fl'] = is_associated_fl\n",
    "    fl_detective = fl_detective.drop(delete_index) \n",
    "    \n",
    "    print 'original number of events:  %d' % num_events\n",
    "    print 'duplicated events deleted:  %d' % num_del\n",
    "    print 'duplicated events merged:  %d' % (len(delete_index)-num_del)\n",
    "    print 'new number of events:  %d' % (num_events-len(delete_index))\n",
    "    print 'actual new number of events:  %d' %fl_detective.shape[0]\n",
    "\n",
    "    return fl_detective     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original number of events:  87044\n",
      "duplicated events deleted:  4077\n",
      "duplicated events merged:  36800\n",
      "new number of events:  46167\n",
      "actual new number of events:  46167\n"
     ]
    }
   ],
   "source": [
    "flare_keywords = list(np.genfromtxt('keywords_flare.csv', delimiter=',', dtype=str))\n",
    "fl_set = pd.read_csv('raw_detective.csv', delimiter = ',', header = 0, usecols = flare_keywords)\n",
    "fl_set = fl_duplicates_detective(fl_set, 30, 50, 8)\n",
    "flare_keywords_after_merge = list(np.genfromtxt('keywords_flare_after_merge.csv', delimiter=',', dtype=str))\n",
    "fl_set.to_csv('flare_dataset_cleaned.csv', index = False, columns = flare_keywords_after_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_cols(df):\n",
    "    #add a column for distance from flare center to disk center (dist_frm_center)\n",
    "    df = add_dist_frm_center_column(df)\n",
    "    #add column for duration of flare\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#find the distance from center of disk from x, y coordinates (unit = arcsec)\n",
    "def find_dist_frm_center(x, y):\n",
    "    #set the radius of the Sun\n",
    "    r = 966\n",
    "    #calculate z squared\n",
    "    z_sq = (r**2 - x**2 - y**2)\n",
    "    #if loop to prevent imaginary numbers when taking square root of z squared\n",
    "    if z_sq >= 0: z = z_sq**(0.5)\n",
    "    else: z = (-z_sq)**(0.5)\n",
    "    #calculate the distance to center, disk center @ (0, 0, R)\n",
    "    dist = (x**2 + y**2 + (z-r)**2)**(0.5)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_dist_frm_center_column(df):\n",
    "#     dist_frm_center = []\n",
    "    dist_frm_center = map(lambda x, y: find_dist_frm_center(x, y), df['hpc_x'], df['hpc_y'])\n",
    "#     for i in range(df.shape[0]):\n",
    "#         dist = find_dist_frm_center(df['hpc_x'].values[i], df['hpc_y'].values[i])\n",
    "#         dist_frm_center.append(dist)\n",
    "    df.loc[:, 'dist_frm_center'] = dist_frm_center\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_duration(start_time, stop_time):\n",
    "    duration = ((stop_time - start_time).total_seconds())/60\n",
    "    return duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fl_duplicates_SSW1(fl_SSW):\n",
    "    delete_index = []\n",
    "    i = 0\n",
    "    end = map(parse_time, fl_SSW['event_endtime'])\n",
    "    \n",
    "    for i in range(len(fl_SSW['SOL_standard'])-1):\n",
    "        j = i + 1\n",
    "        if fl_SSW['SOL_standard'].values[i] == fl_SSW['SOL_standard'].values[j]:\n",
    "            if end[i] <= end[j]:\n",
    "                delete_index.append(i)\n",
    "            else: delete_index.append(j)\n",
    "    \n",
    "    fl_SSW = fl_SSW.drop(delete_index)\n",
    "    \n",
    "    print len(delete_index)\n",
    "    \n",
    "    return fl_SSW    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fl_SSW = pd.read_csv('raw_SSW.csv', delimiter = ',', header = 0 )\n",
    "fl_duplicates_SSW = fl_duplicates_SSW1(fl_SSW)\n",
    "keywords_SSW = list(np.genfromtxt('keywords_SSW_flare.csv', delimiter=',', dtype=str))\n",
    "fl_duplicates_SSW.to_csv('prepped_SSW.csv', index = False, columns = keywords_SSW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def associate_GOES():\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
